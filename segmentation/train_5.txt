2023-03-17 20:13:08,561 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.9.16 (main, Jan 11 2023, 16:05:54) [GCC 11.2.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 3090
CUDA_HOME: /usr/local/cuda
NVCC: Build cuda_11.7.r11.7/compiler.31294372_0
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.9.0+cu111
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.10.0+cu111
OpenCV: 4.7.0
MMCV: 1.4.2
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.1
MMSegmentation: 0.20.2+bb62813
------------------------------------------------------------

2023-03-17 20:13:08,562 - mmseg - INFO - Distributed training: False
2023-03-17 20:13:09,249 - mmseg - INFO - Config:
num_things_classes = 29
num_stuff_classes = 30
num_classes = 59
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
    type='EncoderDecoderMask2Former',
    pretrained=
    'configs/pascal_context/pretrained/beit_base_patch16_224_pt22k_ft22k.pth',
    backbone=dict(
        type='BEiTAdapter',
        patch_size=16,
        embed_dim=768,
        depth=12,
        num_heads=12,
        mlp_ratio=4,
        qkv_bias=True,
        use_abs_pos_emb=False,
        use_rel_pos_bias=True,
        img_size=480,
        init_values=1e-06,
        drop_path_rate=0.2,
        conv_inplane=64,
        n_points=4,
        deform_num_heads=12,
        cffn_ratio=0.25,
        deform_ratio=0.5,
        interaction_indexes=[[0, 2], [3, 5], [6, 8], [9, 11]]),
    decode_head=dict(
        type='Mask2FormerHead',
        in_channels=[768, 768, 768, 768],
        feat_channels=256,
        out_channels=256,
        in_index=[0, 1, 2, 3],
        num_things_classes=29,
        num_stuff_classes=30,
        num_queries=100,
        num_transformer_feat_level=3,
        pixel_decoder=dict(
            type='MSDeformAttnPixelDecoder',
            num_outs=3,
            norm_cfg=dict(type='GN', num_groups=32),
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_heads=8,
                        num_levels=3,
                        num_points=4,
                        im2col_step=64,
                        dropout=0.0,
                        batch_first=False,
                        norm_cfg=None,
                        init_cfg=None),
                    ffn_cfgs=dict(
                        type='FFN',
                        embed_dims=256,
                        feedforward_channels=1024,
                        num_fcs=2,
                        ffn_drop=0.0,
                        act_cfg=dict(type='ReLU', inplace=True)),
                    operation_order=('self_attn', 'norm', 'ffn', 'norm')),
                init_cfg=None),
            positional_encoding=dict(
                type='SinePositionalEncoding', num_feats=128, normalize=True),
            init_cfg=None),
        enforce_decoder_input_project=False,
        positional_encoding=dict(
            type='SinePositionalEncoding', num_feats=128, normalize=True),
        transformer_decoder=dict(
            type='DetrTransformerDecoder',
            return_intermediate=True,
            num_layers=9,
            transformerlayers=dict(
                type='DetrTransformerDecoderLayer',
                attn_cfgs=dict(
                    type='MultiheadAttention',
                    embed_dims=256,
                    num_heads=8,
                    attn_drop=0.0,
                    proj_drop=0.0,
                    dropout_layer=None,
                    batch_first=False),
                ffn_cfgs=dict(
                    embed_dims=256,
                    feedforward_channels=2048,
                    num_fcs=2,
                    act_cfg=dict(type='ReLU', inplace=True),
                    ffn_drop=0.0,
                    dropout_layer=None,
                    add_identity=True),
                feedforward_channels=2048,
                operation_order=('cross_attn', 'norm', 'self_attn', 'norm',
                                 'ffn', 'norm')),
            init_cfg=None),
        loss_cls=dict(
            type='CrossEntropyLoss',
            use_sigmoid=False,
            loss_weight=2.0,
            reduction='mean',
            class_weight=[
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1
            ]),
        loss_mask=dict(
            type='CrossEntropyLoss',
            use_sigmoid=True,
            reduction='mean',
            loss_weight=5.0),
        loss_dice=dict(
            type='DiceLoss',
            use_sigmoid=True,
            activate=True,
            reduction='mean',
            naive_dice=True,
            eps=1.0,
            loss_weight=5.0)),
    train_cfg=dict(
        num_points=12544,
        oversample_ratio=3.0,
        importance_sample_ratio=0.75,
        assigner=dict(
            type='MaskHungarianAssigner',
            cls_cost=dict(type='ClassificationCost', weight=2.0),
            mask_cost=dict(
                type='CrossEntropyLossCost', weight=5.0, use_sigmoid=True),
            dice_cost=dict(
                type='DiceCost', weight=5.0, pred_act=True, eps=1.0)),
        sampler=dict(type='MaskPseudoSampler')),
    test_cfg=dict(
        panoptic_on=True,
        semantic_on=False,
        instance_on=True,
        max_per_image=100,
        iou_thr=0.8,
        filter_low_score=True,
        mode='slide',
        crop_size=(480, 480),
        stride=(320, 320)),
    init_cfg=None)
dataset_type = 'MODIS_Dataset'
data_root = '0314_MODIS_dataset_sample'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
img_scale = (520, 520)
crop_size = (480, 480)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', reduce_zero_label=False),
    dict(type='Resize', img_scale=(520, 520), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=(480, 480), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(480, 480), pad_val=0, seg_pad_val=255),
    dict(type='ToMask'),
    dict(type='DefaultFormatBundle'),
    dict(
        type='Collect',
        keys=['img', 'gt_semantic_seg', 'gt_masks', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(4096, 520),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='ResizeToMultiple', size_divisor=32),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=4,
    train=dict(
        type='MODIS_Dataset',
        data_root='0314_MODIS_dataset_sample',
        img_dir='images',
        ann_dir='labels',
        split='splits/train.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', reduce_zero_label=False),
            dict(type='Resize', img_scale=(520, 520), ratio_range=(0.5, 2.0)),
            dict(type='RandomCrop', crop_size=(480, 480), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(480, 480), pad_val=0, seg_pad_val=255),
            dict(type='ToMask'),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_semantic_seg', 'gt_masks', 'gt_labels'])
        ]),
    val=dict(
        type='MODIS_Dataset',
        data_root='0314_MODIS_dataset_sample',
        img_dir='images',
        ann_dir='labels',
        split='splits/val.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(4096, 520),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='ResizeToMultiple', size_divisor=32),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='MODIS_Dataset',
        data_root='0314_MODIS_dataset_sample',
        img_dir='images',
        ann_dir='labels',
        split='splits/val.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(4096, 520),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='ResizeToMultiple', size_divisor=32),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = 'configs/pascal_context/pretrained/mask2former_beit_adapter_base_480_40k_pascal_context_59.pth.tar'
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
optimizer = dict(
    type='AdamW',
    lr=3e-05,
    betas=(0.9, 0.999),
    weight_decay=0.05,
    constructor='LayerDecayOptimizerConstructor',
    paramwise_cfg=dict(num_layers=12, layer_decay_rate=0.95))
optimizer_config = dict()
lr_config = dict(
    policy='poly',
    warmup='linear',
    warmup_iters=1500,
    warmup_ratio=1e-06,
    power=1.0,
    min_lr=0.0,
    by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=4000)
checkpoint_config = dict(by_epoch=False, interval=1000, max_keep_ckpts=1)
evaluation = dict(
    interval=4000, metric='mIoU', pre_eval=True, save_best='mIoU')
pretrained = 'configs/pascal_context/pretrained/beit_base_patch16_224_pt22k_ft22k.pth'
work_dir = 'test_5'
gpu_ids = range(0, 1)
auto_resume = False

2023-03-17 20:13:09,255 - mmseg - INFO - Set random seed to 1432186633, deterministic: False
Position interpolate for blocks.0.attn.relative_position_bias_table from 27x27 to 59x59
x = [-28.999948522582926, -24.887606396593682, -21.23237281224599, -17.98343774147309, -15.095638959119245, -12.52883425982421, -10.2473434563158, -8.219452402538689, -6.4169721472313235, -4.814847089910567, -3.390806692387691, -2.125055904388428, -1, 0, 1, 2.125055904388428, 3.390806692387691, 4.814847089910567, 6.4169721472313235, 8.219452402538689, 10.2473434563158, 12.52883425982421, 15.095638959119245, 17.98343774147309, 21.23237281224599, 24.887606396593682, 28.999948522582926]
dx = [-29. -28. -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16.
 -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.
  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.
  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.
  27.  28.  29.]
Position interpolate for blocks.1.attn.relative_position_bias_table from 27x27 to 59x59
x = [-28.999948522582926, -24.887606396593682, -21.23237281224599, -17.98343774147309, -15.095638959119245, -12.52883425982421, -10.2473434563158, -8.219452402538689, -6.4169721472313235, -4.814847089910567, -3.390806692387691, -2.125055904388428, -1, 0, 1, 2.125055904388428, 3.390806692387691, 4.814847089910567, 6.4169721472313235, 8.219452402538689, 10.2473434563158, 12.52883425982421, 15.095638959119245, 17.98343774147309, 21.23237281224599, 24.887606396593682, 28.999948522582926]
dx = [-29. -28. -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16.
 -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.
  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.
  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.
  27.  28.  29.]
Position interpolate for blocks.2.attn.relative_position_bias_table from 27x27 to 59x59
x = [-28.999948522582926, -24.887606396593682, -21.23237281224599, -17.98343774147309, -15.095638959119245, -12.52883425982421, -10.2473434563158, -8.219452402538689, -6.4169721472313235, -4.814847089910567, -3.390806692387691, -2.125055904388428, -1, 0, 1, 2.125055904388428, 3.390806692387691, 4.814847089910567, 6.4169721472313235, 8.219452402538689, 10.2473434563158, 12.52883425982421, 15.095638959119245, 17.98343774147309, 21.23237281224599, 24.887606396593682, 28.999948522582926]
dx = [-29. -28. -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16.
 -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.
  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.
  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.
  27.  28.  29.]
Position interpolate for blocks.3.attn.relative_position_bias_table from 27x27 to 59x59
x = [-28.999948522582926, -24.887606396593682, -21.23237281224599, -17.98343774147309, -15.095638959119245, -12.52883425982421, -10.2473434563158, -8.219452402538689, -6.4169721472313235, -4.814847089910567, -3.390806692387691, -2.125055904388428, -1, 0, 1, 2.125055904388428, 3.390806692387691, 4.814847089910567, 6.4169721472313235, 8.219452402538689, 10.2473434563158, 12.52883425982421, 15.095638959119245, 17.98343774147309, 21.23237281224599, 24.887606396593682, 28.999948522582926]
dx = [-29. -28. -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16.
 -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.
  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.
  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.
  27.  28.  29.]
Position interpolate for blocks.4.attn.relative_position_bias_table from 27x27 to 59x59
x = [-28.999948522582926, -24.887606396593682, -21.23237281224599, -17.98343774147309, -15.095638959119245, -12.52883425982421, -10.2473434563158, -8.219452402538689, -6.4169721472313235, -4.814847089910567, -3.390806692387691, -2.125055904388428, -1, 0, 1, 2.125055904388428, 3.390806692387691, 4.814847089910567, 6.4169721472313235, 8.219452402538689, 10.2473434563158, 12.52883425982421, 15.095638959119245, 17.98343774147309, 21.23237281224599, 24.887606396593682, 28.999948522582926]
dx = [-29. -28. -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16.
 -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.
  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.
  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.
  27.  28.  29.]
Position interpolate for blocks.5.attn.relative_position_bias_table from 27x27 to 59x59
x = [-28.999948522582926, -24.887606396593682, -21.23237281224599, -17.98343774147309, -15.095638959119245, -12.52883425982421, -10.2473434563158, -8.219452402538689, -6.4169721472313235, -4.814847089910567, -3.390806692387691, -2.125055904388428, -1, 0, 1, 2.125055904388428, 3.390806692387691, 4.814847089910567, 6.4169721472313235, 8.219452402538689, 10.2473434563158, 12.52883425982421, 15.095638959119245, 17.98343774147309, 21.23237281224599, 24.887606396593682, 28.999948522582926]
dx = [-29. -28. -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16.
 -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.
  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.
  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.
  27.  28.  29.]
Position interpolate for blocks.6.attn.relative_position_bias_table from 27x27 to 59x59
x = [-28.999948522582926, -24.887606396593682, -21.23237281224599, -17.98343774147309, -15.095638959119245, -12.52883425982421, -10.2473434563158, -8.219452402538689, -6.4169721472313235, -4.814847089910567, -3.390806692387691, -2.125055904388428, -1, 0, 1, 2.125055904388428, 3.390806692387691, 4.814847089910567, 6.4169721472313235, 8.219452402538689, 10.2473434563158, 12.52883425982421, 15.095638959119245, 17.98343774147309, 21.23237281224599, 24.887606396593682, 28.999948522582926]
dx = [-29. -28. -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16.
 -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.
  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.
  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.
  27.  28.  29.]
Position interpolate for blocks.7.attn.relative_position_bias_table from 27x27 to 59x59
x = [-28.999948522582926, -24.887606396593682, -21.23237281224599, -17.98343774147309, -15.095638959119245, -12.52883425982421, -10.2473434563158, -8.219452402538689, -6.4169721472313235, -4.814847089910567, -3.390806692387691, -2.125055904388428, -1, 0, 1, 2.125055904388428, 3.390806692387691, 4.814847089910567, 6.4169721472313235, 8.219452402538689, 10.2473434563158, 12.52883425982421, 15.095638959119245, 17.98343774147309, 21.23237281224599, 24.887606396593682, 28.999948522582926]
dx = [-29. -28. -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16.
 -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.
  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.
  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.
  27.  28.  29.]
Position interpolate for blocks.8.attn.relative_position_bias_table from 27x27 to 59x59
x = [-28.999948522582926, -24.887606396593682, -21.23237281224599, -17.98343774147309, -15.095638959119245, -12.52883425982421, -10.2473434563158, -8.219452402538689, -6.4169721472313235, -4.814847089910567, -3.390806692387691, -2.125055904388428, -1, 0, 1, 2.125055904388428, 3.390806692387691, 4.814847089910567, 6.4169721472313235, 8.219452402538689, 10.2473434563158, 12.52883425982421, 15.095638959119245, 17.98343774147309, 21.23237281224599, 24.887606396593682, 28.999948522582926]
dx = [-29. -28. -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16.
 -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.
  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.
  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.
  27.  28.  29.]
Position interpolate for blocks.9.attn.relative_position_bias_table from 27x27 to 59x59
2023-03-17 20:13:11,894 - mmseg - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc_norm.weight, fc_norm.bias, head.weight, head.bias

missing keys in source state_dict: blocks.0.attn.relative_position_index, blocks.1.attn.relative_position_index, blocks.2.attn.relative_position_index, blocks.3.attn.relative_position_index, blocks.4.attn.relative_position_index, blocks.5.attn.relative_position_index, blocks.6.attn.relative_position_index, blocks.7.attn.relative_position_index, blocks.8.attn.relative_position_index, blocks.9.attn.relative_position_index, blocks.10.attn.relative_position_index, blocks.11.attn.relative_position_index

/home/jaehoonhahm/ViT-Adapter-Segmentation/segmentation/mmseg_custom/models/losses/cross_entropy_loss.py:230: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.
  warnings.warn(
/home/jaehoonhahm/ViT-Adapter-Segmentation/segmentation/train.py:179: UserWarning: SyncBN is only supported with DDP. To be compatible with DP, we convert SyncBN to BN. Please use dist_train.sh which can avoid this error.
  warnings.warn(
2023-03-17 20:13:12,535 - mmseg - INFO - EncoderDecoderMask2Former(
  (backbone): BEiTAdapter(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.0181818176060915)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.036363635212183)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.05454545468091965)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.072727270424366)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.09090908616781235)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.10909091681241989)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.12727272510528564)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1454545557498932)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.16363637149333954)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.1818181872367859)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(p=0.20000000298023224)
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (spm): SpatialPriorModule(
      (stem): Sequential(
        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (7): _BatchNormXd(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (8): ReLU(inplace=True)
        (9): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      )
      (conv2): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): _BatchNormXd(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv3): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (conv4): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): _BatchNormXd(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (fc1): Conv2d(64, 768, kernel_size=(1, 1), stride=(1, 1))
      (fc2): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1))
      (fc3): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))
      (fc4): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))
    )
    (interactions): Sequential(
      (0): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=768, out_features=288, bias=True)
            (attention_weights): Linear(in_features=768, out_features=144, bias=True)
            (value_proj): Linear(in_features=768, out_features=384, bias=True)
            (output_proj): Linear(in_features=384, out_features=768, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=768, out_features=96, bias=True)
            (attention_weights): Linear(in_features=768, out_features=48, bias=True)
            (value_proj): Linear(in_features=768, out_features=384, bias=True)
            (output_proj): Linear(in_features=384, out_features=768, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=768, out_features=192, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
            )
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (1): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=768, out_features=288, bias=True)
            (attention_weights): Linear(in_features=768, out_features=144, bias=True)
            (value_proj): Linear(in_features=768, out_features=384, bias=True)
            (output_proj): Linear(in_features=384, out_features=768, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=768, out_features=96, bias=True)
            (attention_weights): Linear(in_features=768, out_features=48, bias=True)
            (value_proj): Linear(in_features=768, out_features=384, bias=True)
            (output_proj): Linear(in_features=384, out_features=768, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=768, out_features=192, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
            )
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (2): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=768, out_features=288, bias=True)
            (attention_weights): Linear(in_features=768, out_features=144, bias=True)
            (value_proj): Linear(in_features=768, out_features=384, bias=True)
            (output_proj): Linear(in_features=384, out_features=768, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=768, out_features=96, bias=True)
            (attention_weights): Linear(in_features=768, out_features=48, bias=True)
            (value_proj): Linear(in_features=768, out_features=384, bias=True)
            (output_proj): Linear(in_features=384, out_features=768, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=768, out_features=192, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
            )
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
      )
      (3): InteractionBlockWithCls(
        (injector): Injector(
          (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=768, out_features=288, bias=True)
            (attention_weights): Linear(in_features=768, out_features=144, bias=True)
            (value_proj): Linear(in_features=768, out_features=384, bias=True)
            (output_proj): Linear(in_features=384, out_features=768, bias=True)
          )
        )
        (extractor): Extractor(
          (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MSDeformAttn(
            (sampling_offsets): Linear(in_features=768, out_features=96, bias=True)
            (attention_weights): Linear(in_features=768, out_features=48, bias=True)
            (value_proj): Linear(in_features=768, out_features=384, bias=True)
            (output_proj): Linear(in_features=384, out_features=768, bias=True)
          )
          (ffn): ConvFFN(
            (fc1): Linear(in_features=768, out_features=192, bias=True)
            (dwconv): DWConv(
              (dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
            )
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (drop_path): DropPath()
        )
        (extra_extractors): Sequential(
          (0): Extractor(
            (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=768, out_features=96, bias=True)
              (attention_weights): Linear(in_features=768, out_features=48, bias=True)
              (value_proj): Linear(in_features=768, out_features=384, bias=True)
              (output_proj): Linear(in_features=384, out_features=768, bias=True)
            )
            (ffn): ConvFFN(
              (fc1): Linear(in_features=768, out_features=192, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              )
              (act): GELU()
              (fc2): Linear(in_features=192, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (drop_path): DropPath()
          )
          (1): Extractor(
            (query_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (feat_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=768, out_features=96, bias=True)
              (attention_weights): Linear(in_features=768, out_features=48, bias=True)
              (value_proj): Linear(in_features=768, out_features=384, bias=True)
              (output_proj): Linear(in_features=384, out_features=768, bias=True)
            )
            (ffn): ConvFFN(
              (fc1): Linear(in_features=768, out_features=192, bias=True)
              (dwconv): DWConv(
                (dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              )
              (act): GELU()
              (fc2): Linear(in_features=192, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (drop_path): DropPath()
          )
        )
      )
    )
    (up): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))
    (norm1): _BatchNormXd(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm2): _BatchNormXd(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm3): _BatchNormXd(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm4): _BatchNormXd(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (decode_head): Mask2FormerHead(
    input_transform=multiple_select, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)
    (conv_seg): None
    (dropout): Dropout2d(p=0.1, inplace=False)
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): ConvModule(
          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): ConvModule(
          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
          (gn): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (encoder): DetrTransformerEncoder(
        (layers): ModuleList(
          (0): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BaseTransformerLayer(
            (attentions): ModuleList(
              (0): MultiScaleDeformableAttention(
                (dropout): Dropout(p=0.0, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (postional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
      (level_encoding): Embedding(3, 256)
      (lateral_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (gn): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (output_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (gn): GroupNorm(32, 256, eps=1e-05, affine=True)
          (activate): ReLU(inplace=True)
        )
      )
      (mask_feature): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (transformer_decoder): DetrTransformerDecoder(
      (layers): ModuleList(
        (0): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (6): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (7): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (8): DetrTransformerDecoderLayer(
          (attentions): ModuleList(
            (0): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
            (1): MultiheadAttention(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
              (dropout_layer): Identity()
            )
          )
          (ffns): ModuleList(
            (0): FFN(
              (activate): ReLU(inplace=True)
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2048, out_features=256, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): Identity()
            )
          )
          (norms): ModuleList(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (decoder_input_projs): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
    )
    (decoder_positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (query_embed): Embedding(100, 256)
    (query_feat): Embedding(100, 256)
    (level_embed): Embedding(3, 256)
    (cls_embed): Linear(in_features=256, out_features=60, bias=True)
    (mask_embed): Sequential(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ReLU(inplace=True)
      (4): Linear(in_features=256, out_features=256, bias=True)
    )
    (loss_cls): CrossEntropyLoss(avg_non_ignore=False)
    (loss_mask): CrossEntropyLoss(avg_non_ignore=False)
    (loss_dice): DiceLoss()
  )
)
2023-03-17 20:13:12,574 - mmseg - INFO - Loaded 831 images
x = [-28.999948522582926, -24.887606396593682, -21.23237281224599, -17.98343774147309, -15.095638959119245, -12.52883425982421, -10.2473434563158, -8.219452402538689, -6.4169721472313235, -4.814847089910567, -3.390806692387691, -2.125055904388428, -1, 0, 1, 2.125055904388428, 3.390806692387691, 4.814847089910567, 6.4169721472313235, 8.219452402538689, 10.2473434563158, 12.52883425982421, 15.095638959119245, 17.98343774147309, 21.23237281224599, 24.887606396593682, 28.999948522582926]
dx = [-29. -28. -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16.
 -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.
  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.
  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.
  27.  28.  29.]
Position interpolate for blocks.10.attn.relative_position_bias_table from 27x27 to 59x59
x = [-28.999948522582926, -24.887606396593682, -21.23237281224599, -17.98343774147309, -15.095638959119245, -12.52883425982421, -10.2473434563158, -8.219452402538689, -6.4169721472313235, -4.814847089910567, -3.390806692387691, -2.125055904388428, -1, 0, 1, 2.125055904388428, 3.390806692387691, 4.814847089910567, 6.4169721472313235, 8.219452402538689, 10.2473434563158, 12.52883425982421, 15.095638959119245, 17.98343774147309, 21.23237281224599, 24.887606396593682, 28.999948522582926]
dx = [-29. -28. -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16.
 -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.
  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.
  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.
  27.  28.  29.]
Position interpolate for blocks.11.attn.relative_position_bias_table from 27x27 to 59x59
x = [-28.999948522582926, -24.887606396593682, -21.23237281224599, -17.98343774147309, -15.095638959119245, -12.52883425982421, -10.2473434563158, -8.219452402538689, -6.4169721472313235, -4.814847089910567, -3.390806692387691, -2.125055904388428, -1, 0, 1, 2.125055904388428, 3.390806692387691, 4.814847089910567, 6.4169721472313235, 8.219452402538689, 10.2473434563158, 12.52883425982421, 15.095638959119245, 17.98343774147309, 21.23237281224599, 24.887606396593682, 28.999948522582926]
dx = [-29. -28. -27. -26. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16.
 -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.
  -1.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.
  13.  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.
  27.  28.  29.]
{'num_layers': 12, 'layer_decay_rate': 0.95}
Build LayerDecayOptimizerConstructor 0.950000 - 14
Param groups = {
  "layer_0_decay": {
    "param_names": [
      "backbone.cls_token",
      "backbone.patch_embed.proj.weight",
      "decode_head.query_embed.weight",
      "decode_head.query_feat.weight",
      "decode_head.level_embed.weight",
      "decode_head.cls_embed.weight",
      "decode_head.mask_embed.0.weight",
      "decode_head.mask_embed.2.weight",
      "decode_head.mask_embed.4.weight"
    ],
    "lr_scale": 0.5133420832795048,
    "lr": 1.5400262498385145e-05,
    "weight_decay": 0.05
  },
  "layer_13_decay": {
    "param_names": [
      "backbone.level_embed",
      "backbone.spm.stem.0.weight",
      "backbone.spm.stem.3.weight",
      "backbone.spm.stem.6.weight",
      "backbone.spm.conv2.0.weight",
      "backbone.spm.conv3.0.weight",
      "backbone.spm.conv4.0.weight",
      "backbone.spm.fc1.weight",
      "backbone.spm.fc2.weight",
      "backbone.spm.fc3.weight",
      "backbone.spm.fc4.weight",
      "backbone.interactions.0.injector.attn.sampling_offsets.weight",
      "backbone.interactions.0.injector.attn.attention_weights.weight",
      "backbone.interactions.0.injector.attn.value_proj.weight",
      "backbone.interactions.0.injector.attn.output_proj.weight",
      "backbone.interactions.0.extractor.attn.sampling_offsets.weight",
      "backbone.interactions.0.extractor.attn.attention_weights.weight",
      "backbone.interactions.0.extractor.attn.value_proj.weight",
      "backbone.interactions.0.extractor.attn.output_proj.weight",
      "backbone.interactions.0.extractor.ffn.fc1.weight",
      "backbone.interactions.0.extractor.ffn.dwconv.dwconv.weight",
      "backbone.interactions.0.extractor.ffn.fc2.weight",
      "backbone.interactions.1.injector.attn.sampling_offsets.weight",
      "backbone.interactions.1.injector.attn.attention_weights.weight",
      "backbone.interactions.1.injector.attn.value_proj.weight",
      "backbone.interactions.1.injector.attn.output_proj.weight",
      "backbone.interactions.1.extractor.attn.sampling_offsets.weight",
      "backbone.interactions.1.extractor.attn.attention_weights.weight",
      "backbone.interactions.1.extractor.attn.value_proj.weight",
      "backbone.interactions.1.extractor.attn.output_proj.weight",
      "backbone.interactions.1.extractor.ffn.fc1.weight",
      "backbone.interactions.1.extractor.ffn.dwconv.dwconv.weight",
      "backbone.interactions.1.extractor.ffn.fc2.weight",
      "backbone.interactions.2.injector.attn.sampling_offsets.weight",
      "backbone.interactions.2.injector.attn.attention_weights.weight",
      "backbone.interactions.2.injector.attn.value_proj.weight",
      "backbone.interactions.2.injector.attn.output_proj.weight",
      "backbone.interactions.2.extractor.attn.sampling_offsets.weight",
      "backbone.interactions.2.extractor.attn.attention_weights.weight",
      "backbone.interactions.2.extractor.attn.value_proj.weight",
      "backbone.interactions.2.extractor.attn.output_proj.weight",
      "backbone.interactions.2.extractor.ffn.fc1.weight",
      "backbone.interactions.2.extractor.ffn.dwconv.dwconv.weight",
      "backbone.interactions.2.extractor.ffn.fc2.weight",
      "backbone.interactions.3.injector.attn.sampling_offsets.weight",
      "backbone.interactions.3.injector.attn.attention_weights.weight",
      "backbone.interactions.3.injector.attn.value_proj.weight",
      "backbone.interactions.3.injector.attn.output_proj.weight",
      "backbone.interactions.3.extractor.attn.sampling_offsets.weight",
      "backbone.interactions.3.extractor.attn.attention_weights.weight",
      "backbone.interactions.3.extractor.attn.value_proj.weight",
      "backbone.interactions.3.extractor.attn.output_proj.weight",
      "backbone.interactions.3.extractor.ffn.fc1.weight",
      "backbone.interactions.3.extractor.ffn.dwconv.dwconv.weight",
      "backbone.interactions.3.extractor.ffn.fc2.weight",
      "backbone.interactions.3.extra_extractors.0.attn.sampling_offsets.weight",
      "backbone.interactions.3.extra_extractors.0.attn.attention_weights.weight",
      "backbone.interactions.3.extra_extractors.0.attn.value_proj.weight",
      "backbone.interactions.3.extra_extractors.0.attn.output_proj.weight",
      "backbone.interactions.3.extra_extractors.0.ffn.fc1.weight",
      "backbone.interactions.3.extra_extractors.0.ffn.dwconv.dwconv.weight",
      "backbone.interactions.3.extra_extractors.0.ffn.fc2.weight",
      "backbone.interactions.3.extra_extractors.1.attn.sampling_offsets.weight",
      "backbone.interactions.3.extra_extractors.1.attn.attention_weights.weight",
      "backbone.interactions.3.extra_extractors.1.attn.value_proj.weight",
      "backbone.interactions.3.extra_extractors.1.attn.output_proj.weight",
      "backbone.interactions.3.extra_extractors.1.ffn.fc1.weight",
      "backbone.interactions.3.extra_extractors.1.ffn.dwconv.dwconv.weight",
      "backbone.interactions.3.extra_extractors.1.ffn.fc2.weight",
      "backbone.up.weight",
      "decode_head.pixel_decoder.input_convs.0.conv.weight",
      "decode_head.pixel_decoder.input_convs.1.conv.weight",
      "decode_head.pixel_decoder.input_convs.2.conv.weight",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.sampling_offsets.weight",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.attention_weights.weight",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.value_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.output_proj.weight",
      "decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.0.0.weight",
      "decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.1.weight",
      "decode_head.pixel_decoder.level_encoding.weight",
      "decode_head.pixel_decoder.lateral_convs.0.conv.weight",
      "decode_head.pixel_decoder.output_convs.0.conv.weight",
      "decode_head.pixel_decoder.mask_feature.weight",
      "decode_head.transformer_decoder.layers.0.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.0.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.0.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.0.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.0.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.0.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.1.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.1.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.1.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.1.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.1.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.1.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.2.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.2.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.2.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.2.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.2.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.2.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.3.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.3.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.3.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.3.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.3.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.3.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.4.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.4.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.4.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.4.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.4.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.4.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.5.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.5.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.5.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.5.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.5.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.5.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.6.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.6.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.6.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.6.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.6.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.6.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.7.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.7.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.7.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.7.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.7.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.7.ffns.0.layers.1.weight",
      "decode_head.transformer_decoder.layers.8.attentions.0.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.8.attentions.0.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.8.attentions.1.attn.in_proj_weight",
      "decode_head.transformer_decoder.layers.8.attentions.1.attn.out_proj.weight",
      "decode_head.transformer_decoder.layers.8.ffns.0.layers.0.0.weight",
      "decode_head.transformer_decoder.layers.8.ffns.0.layers.1.weight"
    ],
    "lr_scale": 1.0,
    "lr": 3e-05,
    "weight_decay": 0.05
  },
  "layer_0_no_decay": {
    "param_names": [
      "backbone.patch_embed.proj.bias",
      "decode_head.cls_embed.bias",
      "decode_head.mask_embed.0.bias",
      "decode_head.mask_embed.2.bias",
      "decode_head.mask_embed.4.bias"
    ],
    "lr_scale": 0.5133420832795048,
    "lr": 1.5400262498385145e-05,
    "weight_decay": 0.0
  },
  "layer_1_no_decay": {
    "param_names": [
      "backbone.blocks.0.gamma_1",
      "backbone.blocks.0.gamma_2",
      "backbone.blocks.0.norm1.weight",
      "backbone.blocks.0.norm1.bias",
      "backbone.blocks.0.attn.q_bias",
      "backbone.blocks.0.attn.v_bias",
      "backbone.blocks.0.attn.proj.bias",
      "backbone.blocks.0.norm2.weight",
      "backbone.blocks.0.norm2.bias",
      "backbone.blocks.0.mlp.fc1.bias",
      "backbone.blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.5403600876626367,
    "lr": 1.62108026298791e-05,
    "weight_decay": 0.0
  },
  "layer_1_decay": {
    "param_names": [
      "backbone.blocks.0.attn.relative_position_bias_table",
      "backbone.blocks.0.attn.qkv.weight",
      "backbone.blocks.0.attn.proj.weight",
      "backbone.blocks.0.mlp.fc1.weight",
      "backbone.blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.5403600876626367,
    "lr": 1.62108026298791e-05,
    "weight_decay": 0.05
  },
  "layer_2_no_decay": {
    "param_names": [
      "backbone.blocks.1.gamma_1",
      "backbone.blocks.1.gamma_2",
      "backbone.blocks.1.norm1.weight",
      "backbone.blocks.1.norm1.bias",
      "backbone.blocks.1.attn.q_bias",
      "backbone.blocks.1.attn.v_bias",
      "backbone.blocks.1.attn.proj.bias",
      "backbone.blocks.1.norm2.weight",
      "backbone.blocks.1.norm2.bias",
      "backbone.blocks.1.mlp.fc1.bias",
      "backbone.blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.5688000922764597,
    "lr": 1.706400276829379e-05,
    "weight_decay": 0.0
  },
  "layer_2_decay": {
    "param_names": [
      "backbone.blocks.1.attn.relative_position_bias_table",
      "backbone.blocks.1.attn.qkv.weight",
      "backbone.blocks.1.attn.proj.weight",
      "backbone.blocks.1.mlp.fc1.weight",
      "backbone.blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.5688000922764597,
    "lr": 1.706400276829379e-05,
    "weight_decay": 0.05
  },
  "layer_3_no_decay": {
    "param_names": [
      "backbone.blocks.2.gamma_1",
      "backbone.blocks.2.gamma_2",
      "backbone.blocks.2.norm1.weight",
      "backbone.blocks.2.norm1.bias",
      "backbone.blocks.2.attn.q_bias",
      "backbone.blocks.2.attn.v_bias",
      "backbone.blocks.2.attn.proj.bias",
      "backbone.blocks.2.norm2.weight",
      "backbone.blocks.2.norm2.bias",
      "backbone.blocks.2.mlp.fc1.bias",
      "backbone.blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.5987369392383787,
    "lr": 1.796210817715136e-05,
    "weight_decay": 0.0
  },
  "layer_3_decay": {
    "param_names": [
      "backbone.blocks.2.attn.relative_position_bias_table",
      "backbone.blocks.2.attn.qkv.weight",
      "backbone.blocks.2.attn.proj.weight",
      "backbone.blocks.2.mlp.fc1.weight",
      "backbone.blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.5987369392383787,
    "lr": 1.796210817715136e-05,
    "weight_decay": 0.05
  },
  "layer_4_no_decay": {
    "param_names": [
      "backbone.blocks.3.gamma_1",
      "backbone.blocks.3.gamma_2",
      "backbone.blocks.3.norm1.weight",
      "backbone.blocks.3.norm1.bias",
      "backbone.blocks.3.attn.q_bias",
      "backbone.blocks.3.attn.v_bias",
      "backbone.blocks.3.attn.proj.bias",
      "backbone.blocks.3.norm2.weight",
      "backbone.blocks.3.norm2.bias",
      "backbone.blocks.3.mlp.fc1.bias",
      "backbone.blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.6302494097246091,
    "lr": 1.890748229173827e-05,
    "weight_decay": 0.0
  },
  "layer_4_decay": {
    "param_names": [
      "backbone.blocks.3.attn.relative_position_bias_table",
      "backbone.blocks.3.attn.qkv.weight",
      "backbone.blocks.3.attn.proj.weight",
      "backbone.blocks.3.mlp.fc1.weight",
      "backbone.blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.6302494097246091,
    "lr": 1.890748229173827e-05,
    "weight_decay": 0.05
  },
  "layer_5_no_decay": {
    "param_names": [
      "backbone.blocks.4.gamma_1",
      "backbone.blocks.4.gamma_2",
      "backbone.blocks.4.norm1.weight",
      "backbone.blocks.4.norm1.bias",
      "backbone.blocks.4.attn.q_bias",
      "backbone.blocks.4.attn.v_bias",
      "backbone.blocks.4.attn.proj.bias",
      "backbone.blocks.4.norm2.weight",
      "backbone.blocks.4.norm2.bias",
      "backbone.blocks.4.mlp.fc1.bias",
      "backbone.blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.6634204312890623,
    "lr": 1.9902612938671867e-05,
    "weight_decay": 0.0
  },
  "layer_5_decay": {
    "param_names": [
      "backbone.blocks.4.attn.relative_position_bias_table",
      "backbone.blocks.4.attn.qkv.weight",
      "backbone.blocks.4.attn.proj.weight",
      "backbone.blocks.4.mlp.fc1.weight",
      "backbone.blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.6634204312890623,
    "lr": 1.9902612938671867e-05,
    "weight_decay": 0.05
  },
  "layer_6_no_decay": {
    "param_names": [
      "backbone.blocks.5.gamma_1",
      "backbone.blocks.5.gamma_2",
      "backbone.blocks.5.norm1.weight",
      "backbone.blocks.5.norm1.bias",
      "backbone.blocks.5.attn.q_bias",
      "backbone.blocks.5.attn.v_bias",
      "backbone.blocks.5.attn.proj.bias",
      "backbone.blocks.5.norm2.weight",
      "backbone.blocks.5.norm2.bias",
      "backbone.blocks.5.mlp.fc1.bias",
      "backbone.blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.6983372960937497,
    "lr": 2.0950118882812494e-05,
    "weight_decay": 0.0
  },
  "layer_6_decay": {
    "param_names": [
      "backbone.blocks.5.attn.relative_position_bias_table",
      "backbone.blocks.5.attn.qkv.weight",
      "backbone.blocks.5.attn.proj.weight",
      "backbone.blocks.5.mlp.fc1.weight",
      "backbone.blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.6983372960937497,
    "lr": 2.0950118882812494e-05,
    "weight_decay": 0.05
  },
  "layer_7_no_decay": {
    "param_names": [
      "backbone.blocks.6.gamma_1",
      "backbone.blocks.6.gamma_2",
      "backbone.blocks.6.norm1.weight",
      "backbone.blocks.6.norm1.bias",
      "backbone.blocks.6.attn.q_bias",
      "backbone.blocks.6.attn.v_bias",
      "backbone.blocks.6.attn.proj.bias",
      "backbone.blocks.6.norm2.weight",
      "backbone.blocks.6.norm2.bias",
      "backbone.blocks.6.mlp.fc1.bias",
      "backbone.blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.7350918906249998,
    "lr": 2.2052756718749993e-05,
    "weight_decay": 0.0
  },
  "layer_7_decay": {
    "param_names": [
      "backbone.blocks.6.attn.relative_position_bias_table",
      "backbone.blocks.6.attn.qkv.weight",
      "backbone.blocks.6.attn.proj.weight",
      "backbone.blocks.6.mlp.fc1.weight",
      "backbone.blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.7350918906249998,
    "lr": 2.2052756718749993e-05,
    "weight_decay": 0.05
  },
  "layer_8_no_decay": {
    "param_names": [
      "backbone.blocks.7.gamma_1",
      "backbone.blocks.7.gamma_2",
      "backbone.blocks.7.norm1.weight",
      "backbone.blocks.7.norm1.bias",
      "backbone.blocks.7.attn.q_bias",
      "backbone.blocks.7.attn.v_bias",
      "backbone.blocks.7.attn.proj.bias",
      "backbone.blocks.7.norm2.weight",
      "backbone.blocks.7.norm2.bias",
      "backbone.blocks.7.mlp.fc1.bias",
      "backbone.blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.7737809374999998,
    "lr": 2.3213428124999993e-05,
    "weight_decay": 0.0
  },
  "layer_8_decay": {
    "param_names": [
      "backbone.blocks.7.attn.relative_position_bias_table",
      "backbone.blocks.7.attn.qkv.weight",
      "backbone.blocks.7.attn.proj.weight",
      "backbone.blocks.7.mlp.fc1.weight",
      "backbone.blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.7737809374999998,
    "lr": 2.3213428124999993e-05,
    "weight_decay": 0.05
  },
  "layer_9_no_decay": {
    "param_names": [
      "backbone.blocks.8.gamma_1",
      "backbone.blocks.8.gamma_2",
      "backbone.blocks.8.norm1.weight",
      "backbone.blocks.8.norm1.bias",
      "backbone.blocks.8.attn.q_bias",
      "backbone.blocks.8.attn.v_bias",
      "backbone.blocks.8.attn.proj.bias",
      "backbone.blocks.8.norm2.weight",
      "backbone.blocks.8.norm2.bias",
      "backbone.blocks.8.mlp.fc1.bias",
      "backbone.blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.8145062499999999,
    "lr": 2.44351875e-05,
    "weight_decay": 0.0
  },
  "layer_9_decay": {
    "param_names": [
      "backbone.blocks.8.attn.relative_position_bias_table",
      "backbone.blocks.8.attn.qkv.weight",
      "backbone.blocks.8.attn.proj.weight",
      "backbone.blocks.8.mlp.fc1.weight",
      "backbone.blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.8145062499999999,
    "lr": 2.44351875e-05,
    "weight_decay": 0.05
  },
  "layer_10_no_decay": {
    "param_names": [
      "backbone.blocks.9.gamma_1",
      "backbone.blocks.9.gamma_2",
      "backbone.blocks.9.norm1.weight",
      "backbone.blocks.9.norm1.bias",
      "backbone.blocks.9.attn.q_bias",
      "backbone.blocks.9.attn.v_bias",
      "backbone.blocks.9.attn.proj.bias",
      "backbone.blocks.9.norm2.weight",
      "backbone.blocks.9.norm2.bias",
      "backbone.blocks.9.mlp.fc1.bias",
      "backbone.blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.8573749999999999,
    "lr": 2.5721249999999997e-05,
    "weight_decay": 0.0
  },
  "layer_10_decay": {
    "param_names": [
      "backbone.blocks.9.attn.relative_position_bias_table",
      "backbone.blocks.9.attn.qkv.weight",
      "backbone.blocks.9.attn.proj.weight",
      "backbone.blocks.9.mlp.fc1.weight",
      "backbone.blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.8573749999999999,
    "lr": 2.5721249999999997e-05,
    "weight_decay": 0.05
  },
  "layer_11_no_decay": {
    "param_names": [
      "backbone.blocks.10.gamma_1",
      "backbone.blocks.10.gamma_2",
      "backbone.blocks.10.norm1.weight",
      "backbone.blocks.10.norm1.bias",
      "backbone.blocks.10.attn.q_bias",
      "backbone.blocks.10.attn.v_bias",
      "backbone.blocks.10.attn.proj.bias",
      "backbone.blocks.10.norm2.weight",
      "backbone.blocks.10.norm2.bias",
      "backbone.blocks.10.mlp.fc1.bias",
      "backbone.blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.9025,
    "lr": 2.7075e-05,
    "weight_decay": 0.0
  },
  "layer_11_decay": {
    "param_names": [
      "backbone.blocks.10.attn.relative_position_bias_table",
      "backbone.blocks.10.attn.qkv.weight",
      "backbone.blocks.10.attn.proj.weight",
      "backbone.blocks.10.mlp.fc1.weight",
      "backbone.blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.9025,
    "lr": 2.7075e-05,
    "weight_decay": 0.05
  },
  "layer_12_no_decay": {
    "param_names": [
      "backbone.blocks.11.gamma_1",
      "backbone.blocks.11.gamma_2",
      "backbone.blocks.11.norm1.weight",
      "backbone.blocks.11.norm1.bias",
      "backbone.blocks.11.attn.q_bias",
      "backbone.blocks.11.attn.v_bias",
      "backbone.blocks.11.attn.proj.bias",
      "backbone.blocks.11.norm2.weight",
      "backbone.blocks.11.norm2.bias",
      "backbone.blocks.11.mlp.fc1.bias",
      "backbone.blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.95,
    "lr": 2.8499999999999998e-05,
    "weight_decay": 0.0
  },
  "layer_12_decay": {
    "param_names": [
      "backbone.blocks.11.attn.relative_position_bias_table",
      "backbone.blocks.11.attn.qkv.weight",
      "backbone.blocks.11.attn.proj.weight",
      "backbone.blocks.11.mlp.fc1.weight",
      "backbone.blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.95,
    "lr": 2.8499999999999998e-05,
    "weight_decay": 0.05
  },
  "layer_13_no_decay": {
    "param_names": [
      "backbone.spm.stem.1.weight",
      "backbone.spm.stem.1.bias",
      "backbone.spm.stem.4.weight",
      "backbone.spm.stem.4.bias",
      "backbone.spm.stem.7.weight",
      "backbone.spm.stem.7.bias",
      "backbone.spm.conv2.1.weight",
      "backbone.spm.conv2.1.bias",
      "backbone.spm.conv3.1.weight",
      "backbone.spm.conv3.1.bias",
      "backbone.spm.conv4.1.weight",
      "backbone.spm.conv4.1.bias",
      "backbone.spm.fc1.bias",
      "backbone.spm.fc2.bias",
      "backbone.spm.fc3.bias",
      "backbone.spm.fc4.bias",
      "backbone.interactions.0.injector.gamma",
      "backbone.interactions.0.injector.query_norm.weight",
      "backbone.interactions.0.injector.query_norm.bias",
      "backbone.interactions.0.injector.feat_norm.weight",
      "backbone.interactions.0.injector.feat_norm.bias",
      "backbone.interactions.0.injector.attn.sampling_offsets.bias",
      "backbone.interactions.0.injector.attn.attention_weights.bias",
      "backbone.interactions.0.injector.attn.value_proj.bias",
      "backbone.interactions.0.injector.attn.output_proj.bias",
      "backbone.interactions.0.extractor.query_norm.weight",
      "backbone.interactions.0.extractor.query_norm.bias",
      "backbone.interactions.0.extractor.feat_norm.weight",
      "backbone.interactions.0.extractor.feat_norm.bias",
      "backbone.interactions.0.extractor.attn.sampling_offsets.bias",
      "backbone.interactions.0.extractor.attn.attention_weights.bias",
      "backbone.interactions.0.extractor.attn.value_proj.bias",
      "backbone.interactions.0.extractor.attn.output_proj.bias",
      "backbone.interactions.0.extractor.ffn.fc1.bias",
      "backbone.interactions.0.extractor.ffn.dwconv.dwconv.bias",
      "backbone.interactions.0.extractor.ffn.fc2.bias",
      "backbone.interactions.0.extractor.ffn_norm.weight",
      "backbone.interactions.0.extractor.ffn_norm.bias",
      "backbone.interactions.1.injector.gamma",
      "backbone.interactions.1.injector.query_norm.weight",
      "backbone.interactions.1.injector.query_norm.bias",
      "backbone.interactions.1.injector.feat_norm.weight",
      "backbone.interactions.1.injector.feat_norm.bias",
      "backbone.interactions.1.injector.attn.sampling_offsets.bias",
      "backbone.interactions.1.injector.attn.attention_weights.bias",
      "backbone.interactions.1.injector.attn.value_proj.bias",
      "backbone.interactions.1.injector.attn.output_proj.bias",
      "backbone.interactions.1.extractor.query_norm.weight",
      "backbone.interactions.1.extractor.query_norm.bias",
      "backbone.interactions.1.extractor.feat_norm.weight",
      "backbone.interactions.1.extractor.feat_norm.bias",
      "backbone.interactions.1.extractor.attn.sampling_offsets.bias",
      "backbone.interactions.1.extractor.attn.attention_weights.bias",
      "backbone.interactions.1.extractor.attn.value_proj.bias",
      "backbone.interactions.1.extractor.attn.output_proj.bias",
      "backbone.interactions.1.extractor.ffn.fc1.bias",
      "backbone.interactions.1.extractor.ffn.dwconv.dwconv.bias",
      "backbone.interactions.1.extractor.ffn.fc2.bias",
      "backbone.interactions.1.extractor.ffn_norm.weight",
      "backbone.interactions.1.extractor.ffn_norm.bias",
      "backbone.interactions.2.injector.gamma",
      "backbone.interactions.2.injector.query_norm.weight",
      "backbone.interactions.2.injector.query_norm.bias",
      "backbone.interactions.2.injector.feat_norm.weight",
      "backbone.interactions.2.injector.feat_norm.bias",
      "backbone.interactions.2.injector.attn.sampling_offsets.bias",
      "backbone.interactions.2.injector.attn.attention_weights.bias",
      "backbone.interactions.2.injector.attn.value_proj.bias",
      "backbone.interactions.2.injector.attn.output_proj.bias",
      "backbone.interactions.2.extractor.query_norm.weight",
      "backbone.interactions.2.extractor.query_norm.bias",
      "backbone.interactions.2.extractor.feat_norm.weight",
      "backbone.interactions.2.extractor.feat_norm.bias",
      "backbone.interactions.2.extractor.attn.sampling_offsets.bias",
      "backbone.interactions.2.extractor.attn.attention_weights.bias",
      "backbone.interactions.2.extractor.attn.value_proj.bias",
      "backbone.interactions.2.extractor.attn.output_proj.bias",
      "backbone.interactions.2.extractor.ffn.fc1.bias",
      "backbone.interactions.2.extractor.ffn.dwconv.dwconv.bias",
      "backbone.interactions.2.extractor.ffn.fc2.bias",
      "backbone.interactions.2.extractor.ffn_norm.weight",
      "backbone.interactions.2.extractor.ffn_norm.bias",
      "backbone.interactions.3.injector.gamma",
      "backbone.interactions.3.injector.query_norm.weight",
      "backbone.interactions.3.injector.query_norm.bias",
      "backbone.interactions.3.injector.feat_norm.weight",
      "backbone.interactions.3.injector.feat_norm.bias",
      "backbone.interactions.3.injector.attn.sampling_offsets.bias",
      "backbone.interactions.3.injector.attn.attention_weights.bias",
      "backbone.interactions.3.injector.attn.value_proj.bias",
      "backbone.interactions.3.injector.attn.output_proj.bias",
      "backbone.interactions.3.extractor.query_norm.weight",
      "backbone.interactions.3.extractor.query_norm.bias",
      "backbone.interactions.3.extractor.feat_norm.weight",
      "backbone.interactions.3.extractor.feat_norm.bias",
      "backbone.interactions.3.extractor.attn.sampling_offsets.bias",
      "backbone.interactions.3.extractor.attn.attention_weights.bias",
      "backbone.interactions.3.extractor.attn.value_proj.bias",
      "backbone.interactions.3.extractor.attn.output_proj.bias",
      "backbone.interactions.3.extractor.ffn.fc1.bias",
      "backbone.interactions.3.extractor.ffn.dwconv.dwconv.bias",
      "backbone.interactions.3.extractor.ffn.fc2.bias",
      "backbone.interactions.3.extractor.ffn_norm.weight",
      "backbone.interactions.3.extractor.ffn_norm.bias",
      "backbone.interactions.3.extra_extractors.0.query_norm.weight",
      "backbone.interactions.3.extra_extractors.0.query_norm.bias",
      "backbone.interactions.3.extra_extractors.0.feat_norm.weight",
      "backbone.interactions.3.extra_extractors.0.feat_norm.bias",
      "backbone.interactions.3.extra_extractors.0.attn.sampling_offsets.bias",
      "backbone.interactions.3.extra_extractors.0.attn.attention_weights.bias",
      "backbone.interactions.3.extra_extractors.0.attn.value_proj.bias",
      "backbone.interactions.3.extra_extractors.0.attn.output_proj.bias",
      "backbone.interactions.3.extra_extractors.0.ffn.fc1.bias",
      "backbone.interactions.3.extra_extractors.0.ffn.dwconv.dwconv.bias",
      "backbone.interactions.3.extra_extractors.0.ffn.fc2.bias",
      "backbone.interactions.3.extra_extractors.0.ffn_norm.weight",
      "backbone.interactions.3.extra_extractors.0.ffn_norm.bias",
      "backbone.interactions.3.extra_extractors.1.query_norm.weight",
      "backbone.interactions.3.extra_extractors.1.query_norm.bias",
      "backbone.interactions.3.extra_extractors.1.feat_norm.weight",
      "backbone.interactions.3.extra_extractors.1.feat_norm.bias",
      "backbone.interactions.3.extra_extractors.1.attn.sampling_offsets.bias",
      "backbone.interactions.3.extra_extractors.1.attn.attention_weights.bias",
      "backbone.interactions.3.extra_extractors.1.attn.value_proj.bias",
      "backbone.interactions.3.extra_extractors.1.attn.output_proj.bias",
      "backbone.interactions.3.extra_extractors.1.ffn.fc1.bias",
      "backbone.interactions.3.extra_extractors.1.ffn.dwconv.dwconv.bias",
      "backbone.interactions.3.extra_extractors.1.ffn.fc2.bias",
      "backbone.interactions.3.extra_extractors.1.ffn_norm.weight",
      "backbone.interactions.3.extra_extractors.1.ffn_norm.bias",
      "backbone.up.bias",
      "backbone.norm1.weight",
      "backbone.norm1.bias",
      "backbone.norm2.weight",
      "backbone.norm2.bias",
      "backbone.norm3.weight",
      "backbone.norm3.bias",
      "backbone.norm4.weight",
      "backbone.norm4.bias",
      "decode_head.pixel_decoder.input_convs.0.conv.bias",
      "decode_head.pixel_decoder.input_convs.0.gn.weight",
      "decode_head.pixel_decoder.input_convs.0.gn.bias",
      "decode_head.pixel_decoder.input_convs.1.conv.bias",
      "decode_head.pixel_decoder.input_convs.1.gn.weight",
      "decode_head.pixel_decoder.input_convs.1.gn.bias",
      "decode_head.pixel_decoder.input_convs.2.conv.bias",
      "decode_head.pixel_decoder.input_convs.2.gn.weight",
      "decode_head.pixel_decoder.input_convs.2.gn.bias",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.0.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.0.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.0.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.0.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.0.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.0.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.1.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.1.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.1.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.1.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.1.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.1.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.2.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.2.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.2.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.2.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.2.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.2.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.3.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.3.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.3.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.3.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.3.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.3.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.4.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.4.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.4.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.4.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.4.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.4.norms.1.bias",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.sampling_offsets.bias",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.attention_weights.bias",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.value_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.5.attentions.0.output_proj.bias",
      "decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.0.0.bias",
      "decode_head.pixel_decoder.encoder.layers.5.ffns.0.layers.1.bias",
      "decode_head.pixel_decoder.encoder.layers.5.norms.0.weight",
      "decode_head.pixel_decoder.encoder.layers.5.norms.0.bias",
      "decode_head.pixel_decoder.encoder.layers.5.norms.1.weight",
      "decode_head.pixel_decoder.encoder.layers.5.norms.1.bias",
      "decode_head.pixel_decoder.lateral_convs.0.gn.weight",
      "decode_head.pixel_decoder.lateral_convs.0.gn.bias",
      "decode_head.pixel_decoder.output_convs.0.gn.weight",
      "decode_head.pixel_decoder.output_convs.0.gn.bias",
      "decode_head.pixel_decoder.mask_feature.bias",
      "decode_head.transformer_decoder.layers.0.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.0.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.0.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.0.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.0.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.0.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.0.norms.0.weight",
      "decode_head.transformer_decoder.layers.0.norms.0.bias",
      "decode_head.transformer_decoder.layers.0.norms.1.weight",
      "decode_head.transformer_decoder.layers.0.norms.1.bias",
      "decode_head.transformer_decoder.layers.0.norms.2.weight",
      "decode_head.transformer_decoder.layers.0.norms.2.bias",
      "decode_head.transformer_decoder.layers.1.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.1.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.1.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.1.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.1.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.1.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.1.norms.0.weight",
      "decode_head.transformer_decoder.layers.1.norms.0.bias",
      "decode_head.transformer_decoder.layers.1.norms.1.weight",
      "decode_head.transformer_decoder.layers.1.norms.1.bias",
      "decode_head.transformer_decoder.layers.1.norms.2.weight",
      "decode_head.transformer_decoder.layers.1.norms.2.bias",
      "decode_head.transformer_decoder.layers.2.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.2.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.2.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.2.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.2.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.2.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.2.norms.0.weight",
      "decode_head.transformer_decoder.layers.2.norms.0.bias",
      "decode_head.transformer_decoder.layers.2.norms.1.weight",
      "decode_head.transformer_decoder.layers.2.norms.1.bias",
      "decode_head.transformer_decoder.layers.2.norms.2.weight",
      "decode_head.transformer_decoder.layers.2.norms.2.bias",
      "decode_head.transformer_decoder.layers.3.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.3.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.3.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.3.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.3.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.3.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.3.norms.0.weight",
      "decode_head.transformer_decoder.layers.3.norms.0.bias",
      "decode_head.transformer_decoder.layers.3.norms.1.weight",
      "decode_head.transformer_decoder.layers.3.norms.1.bias",
      "decode_head.transformer_decoder.layers.3.norms.2.weight",
      "decode_head.transformer_decoder.layers.3.norms.2.bias",
      "decode_head.transformer_decoder.layers.4.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.4.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.4.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.4.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.4.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.4.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.4.norms.0.weight",
      "decode_head.transformer_decoder.layers.4.norms.0.bias",
      "decode_head.transformer_decoder.layers.4.norms.1.weight",
      "decode_head.transformer_decoder.layers.4.norms.1.bias",
      "decode_head.transformer_decoder.layers.4.norms.2.weight",
      "decode_head.transformer_decoder.layers.4.norms.2.bias",
      "decode_head.transformer_decoder.layers.5.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.5.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.5.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.5.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.5.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.5.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.5.norms.0.weight",
      "decode_head.transformer_decoder.layers.5.norms.0.bias",
      "decode_head.transformer_decoder.layers.5.norms.1.weight",
      "decode_head.transformer_decoder.layers.5.norms.1.bias",
      "decode_head.transformer_decoder.layers.5.norms.2.weight",
      "decode_head.transformer_decoder.layers.5.norms.2.bias",
      "decode_head.transformer_decoder.layers.6.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.6.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.6.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.6.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.6.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.6.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.6.norms.0.weight",
      "decode_head.transformer_decoder.layers.6.norms.0.bias",
      "decode_head.transformer_decoder.layers.6.norms.1.weight",
      "decode_head.transformer_decoder.layers.6.norms.1.bias",
      "decode_head.transformer_decoder.layers.6.norms.2.weight",
      "decode_head.transformer_decoder.layers.6.norms.2.bias",
      "decode_head.transformer_decoder.layers.7.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.7.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.7.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.7.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.7.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.7.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.7.norms.0.weight",
      "decode_head.transformer_decoder.layers.7.norms.0.bias",
      "decode_head.transformer_decoder.layers.7.norms.1.weight",
      "decode_head.transformer_decoder.layers.7.norms.1.bias",
      "decode_head.transformer_decoder.layers.7.norms.2.weight",
      "decode_head.transformer_decoder.layers.7.norms.2.bias",
      "decode_head.transformer_decoder.layers.8.attentions.0.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.8.attentions.0.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.8.attentions.1.attn.in_proj_bias",
      "decode_head.transformer_decoder.layers.8.attentions.1.attn.out_proj.bias",
      "decode_head.transformer_decoder.layers.8.ffns.0.layers.0.0.bias",
      "decode_head.transformer_decoder.layers.8.ffns.0.layers.1.bias",
      "decode_head.transformer_decoder.layers.8.norms.0.weight",
      "decode_head.transformer_decoder.layers.8.norms.0.bias",
      "decode_head.transformer_decoder.layers.8.norms.1.weight",
      "decode_head.transformer_decoder.layers.8.norms.1.bias",
      "decode_head.transformer_decoder.layers.8.norms.2.weight",
      "decode_head.transformer_decoder.layers.8.norms.2.bias",
      "decode_head.transformer_decoder.post_norm.weight",
      "decode_head.transformer_decoder.post_norm.bias"
    ],
    "lr_scale": 1.0,
    "lr": 3e-05,
    "weight_decay": 0.0
  }
2023-03-17 20:13:29,824 - mmseg - INFO - Loaded 215 images
2023-03-17 20:13:29,827 - mmseg - INFO - load checkpoint from local path: configs/pascal_context/pretrained/mask2former_beit_adapter_base_480_40k_pascal_context_59.pth.tar
2023-03-17 20:13:31,771 - mmseg - WARNING - The model and loaded state dict do not match exactly

missing keys in source state_dict: backbone.blocks.0.attn.relative_position_index, backbone.blocks.1.attn.relative_position_index, backbone.blocks.2.attn.relative_position_index, backbone.blocks.3.attn.relative_position_index, backbone.blocks.4.attn.relative_position_index, backbone.blocks.5.attn.relative_position_index, backbone.blocks.6.attn.relative_position_index, backbone.blocks.7.attn.relative_position_index, backbone.blocks.8.attn.relative_position_index, backbone.blocks.9.attn.relative_position_index, backbone.blocks.10.attn.relative_position_index, backbone.blocks.11.attn.relative_position_index

2023-03-17 20:13:31,782 - mmseg - INFO - Start running, host: jaehoonhahm@c02, work_dir: /home/jaehoonhahm/ViT-Adapter-Segmentation/segmentation/test_5
2023-03-17 20:13:31,788 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-03-17 20:13:31,792 - mmseg - INFO - workflow: [('train', 1)], max: 4000 iters
2023-03-17 20:13:31,794 - mmseg - INFO - Checkpoints will be saved to /home/jaehoonhahm/ViT-Adapter-Segmentation/segmentation/test_5 by HardDiskBackend.
}
/home/jaehoonhahm/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
/home/jaehoonhahm/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/functional.py:3657: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/jaehoonhahm/anaconda3/envs/seg/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
2023-03-17 20:14:04,582 - mmseg - INFO - Iter [50/4000]	lr: 4.969e-07, eta: 0:42:18, time: 0.643, data_time: 0.022, memory: 14565, decode.loss_cls: 9.3365, decode.loss_mask: 2.5212, decode.loss_dice: 3.2483, decode.d0.loss_cls: 5.9729, decode.d0.loss_mask: 1.9871, decode.d0.loss_dice: 3.3916, decode.d1.loss_cls: 8.5827, decode.d1.loss_mask: 2.1797, decode.d1.loss_dice: 3.2148, decode.d2.loss_cls: 9.3037, decode.d2.loss_mask: 2.2614, decode.d2.loss_dice: 3.2303, decode.d3.loss_cls: 9.7043, decode.d3.loss_mask: 2.3233, decode.d3.loss_dice: 3.2262, decode.d4.loss_cls: 9.5042, decode.d4.loss_mask: 2.3729, decode.d4.loss_dice: 3.2660, decode.d5.loss_cls: 9.3991, decode.d5.loss_mask: 2.4255, decode.d5.loss_dice: 3.2188, decode.d6.loss_cls: 9.1666, decode.d6.loss_mask: 2.4752, decode.d6.loss_dice: 3.2155, decode.d7.loss_cls: 9.1333, decode.d7.loss_mask: 2.4661, decode.d7.loss_dice: 3.2408, decode.d8.loss_cls: 9.1897, decode.d8.loss_mask: 2.5258, decode.d8.loss_dice: 3.2233, loss: 145.3067
2023-03-17 20:14:33,674 - mmseg - INFO - Iter [100/4000]	lr: 9.913e-07, eta: 0:39:47, time: 0.582, data_time: 0.005, memory: 14565, decode.loss_cls: 6.8184, decode.loss_mask: 2.3154, decode.loss_dice: 3.2672, decode.d0.loss_cls: 5.9683, decode.d0.loss_mask: 1.9345, decode.d0.loss_dice: 3.3903, decode.d1.loss_cls: 7.1730, decode.d1.loss_mask: 2.0355, decode.d1.loss_dice: 3.2613, decode.d2.loss_cls: 7.5585, decode.d2.loss_mask: 2.1181, decode.d2.loss_dice: 3.2711, decode.d3.loss_cls: 7.6124, decode.d3.loss_mask: 2.1739, decode.d3.loss_dice: 3.2469, decode.d4.loss_cls: 7.4166, decode.d4.loss_mask: 2.2074, decode.d4.loss_dice: 3.2425, decode.d5.loss_cls: 7.2144, decode.d5.loss_mask: 2.2527, decode.d5.loss_dice: 3.2448, decode.d6.loss_cls: 6.9927, decode.d6.loss_mask: 2.2590, decode.d6.loss_dice: 3.2470, decode.d7.loss_cls: 6.9245, decode.d7.loss_mask: 2.2784, decode.d7.loss_dice: 3.2360, decode.d8.loss_cls: 6.9221, decode.d8.loss_mask: 2.3037, decode.d8.loss_dice: 3.2433, loss: 125.1299
2023-03-17 20:15:03,917 - mmseg - INFO - Iter [150/4000]	lr: 1.473e-06, eta: 0:39:07, time: 0.605, data_time: 0.005, memory: 14565, decode.loss_cls: 3.9449, decode.loss_mask: 1.9180, decode.loss_dice: 3.3479, decode.d0.loss_cls: 5.9686, decode.d0.loss_mask: 1.8744, decode.d0.loss_dice: 3.4263, decode.d1.loss_cls: 5.4738, decode.d1.loss_mask: 1.8845, decode.d1.loss_dice: 3.2513, decode.d2.loss_cls: 5.3841, decode.d2.loss_mask: 1.9271, decode.d2.loss_dice: 3.2797, decode.d3.loss_cls: 4.9088, decode.d3.loss_mask: 1.9075, decode.d3.loss_dice: 3.2730, decode.d4.loss_cls: 4.7056, decode.d4.loss_mask: 1.9297, decode.d4.loss_dice: 3.2972, decode.d5.loss_cls: 4.5017, decode.d5.loss_mask: 1.9690, decode.d5.loss_dice: 3.2867, decode.d6.loss_cls: 4.3250, decode.d6.loss_mask: 1.9524, decode.d6.loss_dice: 3.2970, decode.d7.loss_cls: 4.0397, decode.d7.loss_mask: 1.9307, decode.d7.loss_dice: 3.3480, decode.d8.loss_cls: 3.9659, decode.d8.loss_mask: 1.9158, decode.d8.loss_dice: 3.3571, loss: 99.5914
2023-03-17 20:15:32,474 - mmseg - INFO - Iter [200/4000]	lr: 1.941e-06, eta: 0:38:00, time: 0.571, data_time: 0.005, memory: 14565, decode.loss_cls: 2.7221, decode.loss_mask: 1.7692, decode.loss_dice: 3.4034, decode.d0.loss_cls: 5.9345, decode.d0.loss_mask: 1.7796, decode.d0.loss_dice: 3.3169, decode.d1.loss_cls: 4.4839, decode.d1.loss_mask: 1.7660, decode.d1.loss_dice: 3.2418, decode.d2.loss_cls: 3.9895, decode.d2.loss_mask: 1.7504, decode.d2.loss_dice: 3.2312, decode.d3.loss_cls: 3.5298, decode.d3.loss_mask: 1.7085, decode.d3.loss_dice: 3.2356, decode.d4.loss_cls: 3.1937, decode.d4.loss_mask: 1.7471, decode.d4.loss_dice: 3.2466, decode.d5.loss_cls: 3.0540, decode.d5.loss_mask: 1.7493, decode.d5.loss_dice: 3.2670, decode.d6.loss_cls: 2.9508, decode.d6.loss_mask: 1.7464, decode.d6.loss_dice: 3.3138, decode.d7.loss_cls: 2.7367, decode.d7.loss_mask: 1.7222, decode.d7.loss_dice: 3.3501, decode.d8.loss_cls: 2.6991, decode.d8.loss_mask: 1.7284, decode.d8.loss_dice: 3.3741, loss: 85.7416
2023-03-17 20:16:05,115 - mmseg - INFO - Iter [250/4000]	lr: 2.397e-06, eta: 0:38:09, time: 0.653, data_time: 0.005, memory: 14565, decode.loss_cls: 2.1576, decode.loss_mask: 1.6083, decode.loss_dice: 3.2539, decode.d0.loss_cls: 5.9681, decode.d0.loss_mask: 1.6464, decode.d0.loss_dice: 3.2293, decode.d1.loss_cls: 3.7488, decode.d1.loss_mask: 1.6234, decode.d1.loss_dice: 3.1697, decode.d2.loss_cls: 3.2566, decode.d2.loss_mask: 1.6044, decode.d2.loss_dice: 3.2428, decode.d3.loss_cls: 2.7605, decode.d3.loss_mask: 1.5939, decode.d3.loss_dice: 3.2381, decode.d4.loss_cls: 2.4846, decode.d4.loss_mask: 1.6018, decode.d4.loss_dice: 3.2314, decode.d5.loss_cls: 2.3023, decode.d5.loss_mask: 1.5776, decode.d5.loss_dice: 3.2508, decode.d6.loss_cls: 2.2666, decode.d6.loss_mask: 1.5782, decode.d6.loss_dice: 3.2215, decode.d7.loss_cls: 2.1686, decode.d7.loss_mask: 1.5632, decode.d7.loss_dice: 3.2425, decode.d8.loss_cls: 2.1631, decode.d8.loss_mask: 1.5920, decode.d8.loss_dice: 3.2409, loss: 77.5870
2023-03-17 20:16:34,467 - mmseg - INFO - Iter [300/4000]	lr: 2.840e-06, eta: 0:37:24, time: 0.587, data_time: 0.005, memory: 14565, decode.loss_cls: 1.5590, decode.loss_mask: 1.5863, decode.loss_dice: 3.2272, decode.d0.loss_cls: 5.8792, decode.d0.loss_mask: 1.6014, decode.d0.loss_dice: 3.2321, decode.d1.loss_cls: 3.2907, decode.d1.loss_mask: 1.5812, decode.d1.loss_dice: 3.1852, decode.d2.loss_cls: 2.6317, decode.d2.loss_mask: 1.5730, decode.d2.loss_dice: 3.2121, decode.d3.loss_cls: 2.0562, decode.d3.loss_mask: 1.5836, decode.d3.loss_dice: 3.2301, decode.d4.loss_cls: 1.7496, decode.d4.loss_mask: 1.5847, decode.d4.loss_dice: 3.2427, decode.d5.loss_cls: 1.5826, decode.d5.loss_mask: 1.5691, decode.d5.loss_dice: 3.2407, decode.d6.loss_cls: 1.6007, decode.d6.loss_mask: 1.5876, decode.d6.loss_dice: 3.2228, decode.d7.loss_cls: 1.5583, decode.d7.loss_mask: 1.5768, decode.d7.loss_dice: 3.2467, decode.d8.loss_cls: 1.5565, decode.d8.loss_mask: 1.5759, decode.d8.loss_dice: 3.2282, loss: 71.5520
2023-03-17 20:17:03,227 - mmseg - INFO - Iter [350/4000]	lr: 3.271e-06, eta: 0:36:38, time: 0.575, data_time: 0.005, memory: 14565, decode.loss_cls: 1.1442, decode.loss_mask: 1.5477, decode.loss_dice: 3.2275, decode.d0.loss_cls: 5.9132, decode.d0.loss_mask: 1.5487, decode.d0.loss_dice: 3.2157, decode.d1.loss_cls: 3.0144, decode.d1.loss_mask: 1.5709, decode.d1.loss_dice: 3.2096, decode.d2.loss_cls: 2.1811, decode.d2.loss_mask: 1.5460, decode.d2.loss_dice: 3.2457, decode.d3.loss_cls: 1.5971, decode.d3.loss_mask: 1.5536, decode.d3.loss_dice: 3.2259, decode.d4.loss_cls: 1.3108, decode.d4.loss_mask: 1.5741, decode.d4.loss_dice: 3.2236, decode.d5.loss_cls: 1.1106, decode.d5.loss_mask: 1.5569, decode.d5.loss_dice: 3.2031, decode.d6.loss_cls: 1.1008, decode.d6.loss_mask: 1.5649, decode.d6.loss_dice: 3.2151, decode.d7.loss_cls: 1.0850, decode.d7.loss_mask: 1.5579, decode.d7.loss_dice: 3.1903, decode.d8.loss_cls: 1.1135, decode.d8.loss_mask: 1.5564, decode.d8.loss_dice: 3.1905, loss: 67.2947
2023-03-17 20:17:32,140 - mmseg - INFO - Iter [400/4000]	lr: 3.688e-06, eta: 0:35:57, time: 0.578, data_time: 0.005, memory: 14565, decode.loss_cls: 0.7342, decode.loss_mask: 1.5118, decode.loss_dice: 3.2754, decode.d0.loss_cls: 5.8993, decode.d0.loss_mask: 1.5143, decode.d0.loss_dice: 3.2471, decode.d1.loss_cls: 2.5919, decode.d1.loss_mask: 1.5070, decode.d1.loss_dice: 3.3010, decode.d2.loss_cls: 1.7259, decode.d2.loss_mask: 1.4983, decode.d2.loss_dice: 3.2809, decode.d3.loss_cls: 1.0956, decode.d3.loss_mask: 1.4964, decode.d3.loss_dice: 3.2884, decode.d4.loss_cls: 0.8138, decode.d4.loss_mask: 1.5187, decode.d4.loss_dice: 3.2952, decode.d5.loss_cls: 0.6861, decode.d5.loss_mask: 1.5124, decode.d5.loss_dice: 3.2797, decode.d6.loss_cls: 0.6870, decode.d6.loss_mask: 1.5263, decode.d6.loss_dice: 3.2573, decode.d7.loss_cls: 0.6888, decode.d7.loss_mask: 1.5116, decode.d7.loss_dice: 3.2658, decode.d8.loss_cls: 0.6976, decode.d8.loss_mask: 1.5185, decode.d8.loss_dice: 3.2617, loss: 63.4879
2023-03-17 20:18:02,871 - mmseg - INFO - Iter [450/4000]	lr: 4.092e-06, eta: 0:35:33, time: 0.615, data_time: 0.049, memory: 14565, decode.loss_cls: 0.4748, decode.loss_mask: 1.5045, decode.loss_dice: 3.2465, decode.d0.loss_cls: 5.8848, decode.d0.loss_mask: 1.5179, decode.d0.loss_dice: 3.2128, decode.d1.loss_cls: 2.3009, decode.d1.loss_mask: 1.5382, decode.d1.loss_dice: 3.2638, decode.d2.loss_cls: 1.3695, decode.d2.loss_mask: 1.4980, decode.d2.loss_dice: 3.2737, decode.d3.loss_cls: 0.8066, decode.d3.loss_mask: 1.4981, decode.d3.loss_dice: 3.2731, decode.d4.loss_cls: 0.5804, decode.d4.loss_mask: 1.5120, decode.d4.loss_dice: 3.2628, decode.d5.loss_cls: 0.4858, decode.d5.loss_mask: 1.4973, decode.d5.loss_dice: 3.2617, decode.d6.loss_cls: 0.4615, decode.d6.loss_mask: 1.5001, decode.d6.loss_dice: 3.2420, decode.d7.loss_cls: 0.4566, decode.d7.loss_mask: 1.4969, decode.d7.loss_dice: 3.2471, decode.d8.loss_cls: 0.4826, decode.d8.loss_mask: 1.5060, decode.d8.loss_dice: 3.2469, loss: 60.9028
2023-03-17 20:18:30,012 - mmseg - INFO - Iter [500/4000]	lr: 4.484e-06, eta: 0:34:42, time: 0.543, data_time: 0.005, memory: 14565, decode.loss_cls: 0.2860, decode.loss_mask: 1.4748, decode.loss_dice: 3.2460, decode.d0.loss_cls: 5.9012, decode.d0.loss_mask: 1.4567, decode.d0.loss_dice: 3.2269, decode.d1.loss_cls: 1.9635, decode.d1.loss_mask: 1.4599, decode.d1.loss_dice: 3.2896, decode.d2.loss_cls: 0.9797, decode.d2.loss_mask: 1.4554, decode.d2.loss_dice: 3.2448, decode.d3.loss_cls: 0.5275, decode.d3.loss_mask: 1.4627, decode.d3.loss_dice: 3.2548, decode.d4.loss_cls: 0.3662, decode.d4.loss_mask: 1.4696, decode.d4.loss_dice: 3.2374, decode.d5.loss_cls: 0.3179, decode.d5.loss_mask: 1.4671, decode.d5.loss_dice: 3.2661, decode.d6.loss_cls: 0.2818, decode.d6.loss_mask: 1.4875, decode.d6.loss_dice: 3.2555, decode.d7.loss_cls: 0.2762, decode.d7.loss_mask: 1.4717, decode.d7.loss_dice: 3.2450, decode.d8.loss_cls: 0.2820, decode.d8.loss_mask: 1.4803, decode.d8.loss_dice: 3.2368, loss: 58.3709
2023-03-17 20:18:57,234 - mmseg - INFO - Iter [550/4000]	lr: 4.863e-06, eta: 0:33:57, time: 0.544, data_time: 0.005, memory: 14565, decode.loss_cls: 0.2294, decode.loss_mask: 1.5264, decode.loss_dice: 3.2710, decode.d0.loss_cls: 5.8027, decode.d0.loss_mask: 1.5078, decode.d0.loss_dice: 3.2394, decode.d1.loss_cls: 1.6328, decode.d1.loss_mask: 1.5094, decode.d1.loss_dice: 3.2791, decode.d2.loss_cls: 0.7058, decode.d2.loss_mask: 1.5234, decode.d2.loss_dice: 3.2761, decode.d3.loss_cls: 0.4150, decode.d3.loss_mask: 1.5305, decode.d3.loss_dice: 3.2685, decode.d4.loss_cls: 0.2895, decode.d4.loss_mask: 1.5298, decode.d4.loss_dice: 3.2569, decode.d5.loss_cls: 0.2437, decode.d5.loss_mask: 1.5310, decode.d5.loss_dice: 3.2749, decode.d6.loss_cls: 0.2494, decode.d6.loss_mask: 1.5353, decode.d6.loss_dice: 3.2806, decode.d7.loss_cls: 0.2296, decode.d7.loss_mask: 1.5410, decode.d7.loss_dice: 3.2650, decode.d8.loss_cls: 0.2264, decode.d8.loss_mask: 1.5378, decode.d8.loss_dice: 3.2493, loss: 57.9576
2023-03-17 20:19:25,278 - mmseg - INFO - Iter [600/4000]	lr: 5.229e-06, eta: 0:33:19, time: 0.561, data_time: 0.005, memory: 14565, decode.loss_cls: 0.1734, decode.loss_mask: 1.5495, decode.loss_dice: 3.2571, decode.d0.loss_cls: 5.7220, decode.d0.loss_mask: 1.5337, decode.d0.loss_dice: 3.2459, decode.d1.loss_cls: 1.2804, decode.d1.loss_mask: 1.5350, decode.d1.loss_dice: 3.2788, decode.d2.loss_cls: 0.4743, decode.d2.loss_mask: 1.5496, decode.d2.loss_dice: 3.2656, decode.d3.loss_cls: 0.2605, decode.d3.loss_mask: 1.5554, decode.d3.loss_dice: 3.2512, decode.d4.loss_cls: 0.2090, decode.d4.loss_mask: 1.5472, decode.d4.loss_dice: 3.2842, decode.d5.loss_cls: 0.1792, decode.d5.loss_mask: 1.5407, decode.d5.loss_dice: 3.2701, decode.d6.loss_cls: 0.1589, decode.d6.loss_mask: 1.5534, decode.d6.loss_dice: 3.2633, decode.d7.loss_cls: 0.1530, decode.d7.loss_mask: 1.5520, decode.d7.loss_dice: 3.2496, decode.d8.loss_cls: 0.1407, decode.d8.loss_mask: 1.5548, decode.d8.loss_dice: 3.2631, loss: 56.8514
2023-03-17 20:19:53,104 - mmseg - INFO - Iter [650/4000]	lr: 5.582e-06, eta: 0:32:41, time: 0.557, data_time: 0.005, memory: 14565, decode.loss_cls: 0.1974, decode.loss_mask: 1.4547, decode.loss_dice: 3.2081, decode.d0.loss_cls: 5.7023, decode.d0.loss_mask: 1.4424, decode.d0.loss_dice: 3.1924, decode.d1.loss_cls: 1.0698, decode.d1.loss_mask: 1.4412, decode.d1.loss_dice: 3.2339, decode.d2.loss_cls: 0.4645, decode.d2.loss_mask: 1.4450, decode.d2.loss_dice: 3.1923, decode.d3.loss_cls: 0.3083, decode.d3.loss_mask: 1.4479, decode.d3.loss_dice: 3.1956, decode.d4.loss_cls: 0.2428, decode.d4.loss_mask: 1.4495, decode.d4.loss_dice: 3.1843, decode.d5.loss_cls: 0.1974, decode.d5.loss_mask: 1.4558, decode.d5.loss_dice: 3.1983, decode.d6.loss_cls: 0.2436, decode.d6.loss_mask: 1.4499, decode.d6.loss_dice: 3.1946, decode.d7.loss_cls: 0.1980, decode.d7.loss_mask: 1.4553, decode.d7.loss_dice: 3.2093, decode.d8.loss_cls: 0.2020, decode.d8.loss_mask: 1.4574, decode.d8.loss_dice: 3.2080, loss: 55.3422
2023-03-17 20:20:21,714 - mmseg - INFO - Iter [700/4000]	lr: 5.922e-06, eta: 0:32:09, time: 0.572, data_time: 0.006, memory: 14565, decode.loss_cls: 0.1184, decode.loss_mask: 1.4941, decode.loss_dice: 3.1873, decode.d0.loss_cls: 5.7557, decode.d0.loss_mask: 1.4709, decode.d0.loss_dice: 3.1768, decode.d1.loss_cls: 0.8504, decode.d1.loss_mask: 1.4821, decode.d1.loss_dice: 3.1735, decode.d2.loss_cls: 0.3408, decode.d2.loss_mask: 1.4831, decode.d2.loss_dice: 3.1786, decode.d3.loss_cls: 0.2065, decode.d3.loss_mask: 1.4913, decode.d3.loss_dice: 3.1949, decode.d4.loss_cls: 0.1734, decode.d4.loss_mask: 1.4861, decode.d4.loss_dice: 3.1764, decode.d5.loss_cls: 0.1420, decode.d5.loss_mask: 1.4770, decode.d5.loss_dice: 3.1798, decode.d6.loss_cls: 0.1429, decode.d6.loss_mask: 1.4837, decode.d6.loss_dice: 3.1697, decode.d7.loss_cls: 0.1307, decode.d7.loss_mask: 1.4859, decode.d7.loss_dice: 3.1768, decode.d8.loss_cls: 0.1158, decode.d8.loss_mask: 1.4907, decode.d8.loss_dice: 3.1879, loss: 54.6232
2023-03-17 20:20:49,359 - mmseg - INFO - Iter [750/4000]	lr: 6.250e-06, eta: 0:31:33, time: 0.553, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0800, decode.loss_mask: 1.4836, decode.loss_dice: 3.1502, decode.d0.loss_cls: 5.6919, decode.d0.loss_mask: 1.4832, decode.d0.loss_dice: 3.1601, decode.d1.loss_cls: 0.6306, decode.d1.loss_mask: 1.4896, decode.d1.loss_dice: 3.1753, decode.d2.loss_cls: 0.2173, decode.d2.loss_mask: 1.4868, decode.d2.loss_dice: 3.1686, decode.d3.loss_cls: 0.1270, decode.d3.loss_mask: 1.4907, decode.d3.loss_dice: 3.1515, decode.d4.loss_cls: 0.0972, decode.d4.loss_mask: 1.4897, decode.d4.loss_dice: 3.1479, decode.d5.loss_cls: 0.0858, decode.d5.loss_mask: 1.4903, decode.d5.loss_dice: 3.1591, decode.d6.loss_cls: 0.0815, decode.d6.loss_mask: 1.4890, decode.d6.loss_dice: 3.1530, decode.d7.loss_cls: 0.0840, decode.d7.loss_mask: 1.4929, decode.d7.loss_dice: 3.1360, decode.d8.loss_cls: 0.0828, decode.d8.loss_mask: 1.4891, decode.d8.loss_dice: 3.1478, loss: 53.6126
2023-03-17 20:21:16,748 - mmseg - INFO - Iter [800/4000]	lr: 6.565e-06, eta: 0:30:57, time: 0.548, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0961, decode.loss_mask: 1.5420, decode.loss_dice: 3.1607, decode.d0.loss_cls: 5.6187, decode.d0.loss_mask: 1.5251, decode.d0.loss_dice: 3.1665, decode.d1.loss_cls: 0.5475, decode.d1.loss_mask: 1.5293, decode.d1.loss_dice: 3.1864, decode.d2.loss_cls: 0.2160, decode.d2.loss_mask: 1.5359, decode.d2.loss_dice: 3.1885, decode.d3.loss_cls: 0.1303, decode.d3.loss_mask: 1.5336, decode.d3.loss_dice: 3.2092, decode.d4.loss_cls: 0.1135, decode.d4.loss_mask: 1.5360, decode.d4.loss_dice: 3.1674, decode.d5.loss_cls: 0.1065, decode.d5.loss_mask: 1.5299, decode.d5.loss_dice: 3.1769, decode.d6.loss_cls: 0.1192, decode.d6.loss_mask: 1.5324, decode.d6.loss_dice: 3.1589, decode.d7.loss_cls: 0.1004, decode.d7.loss_mask: 1.5451, decode.d7.loss_dice: 3.1648, decode.d8.loss_cls: 0.1006, decode.d8.loss_mask: 1.5379, decode.d8.loss_dice: 3.1906, loss: 54.2659
2023-03-17 20:21:46,665 - mmseg - INFO - Iter [850/4000]	lr: 6.866e-06, eta: 0:30:31, time: 0.598, data_time: 0.049, memory: 14565, decode.loss_cls: 0.0987, decode.loss_mask: 1.4984, decode.loss_dice: 3.1940, decode.d0.loss_cls: 5.6563, decode.d0.loss_mask: 1.5030, decode.d0.loss_dice: 3.1735, decode.d1.loss_cls: 0.4554, decode.d1.loss_mask: 1.5020, decode.d1.loss_dice: 3.2262, decode.d2.loss_cls: 0.2007, decode.d2.loss_mask: 1.4954, decode.d2.loss_dice: 3.2049, decode.d3.loss_cls: 0.1489, decode.d3.loss_mask: 1.4891, decode.d3.loss_dice: 3.1972, decode.d4.loss_cls: 0.1181, decode.d4.loss_mask: 1.5036, decode.d4.loss_dice: 3.1904, decode.d5.loss_cls: 0.1162, decode.d5.loss_mask: 1.4885, decode.d5.loss_dice: 3.1833, decode.d6.loss_cls: 0.1090, decode.d6.loss_mask: 1.4974, decode.d6.loss_dice: 3.1876, decode.d7.loss_cls: 0.0876, decode.d7.loss_mask: 1.5045, decode.d7.loss_dice: 3.1816, decode.d8.loss_cls: 0.0889, decode.d8.loss_mask: 1.5002, decode.d8.loss_dice: 3.1866, loss: 53.9874
2023-03-17 20:22:14,576 - mmseg - INFO - Iter [900/4000]	lr: 7.155e-06, eta: 0:29:58, time: 0.558, data_time: 0.005, memory: 14565, decode.loss_cls: 0.1177, decode.loss_mask: 1.5037, decode.loss_dice: 3.1540, decode.d0.loss_cls: 5.5309, decode.d0.loss_mask: 1.4683, decode.d0.loss_dice: 3.1567, decode.d1.loss_cls: 0.4131, decode.d1.loss_mask: 1.4959, decode.d1.loss_dice: 3.1745, decode.d2.loss_cls: 0.2352, decode.d2.loss_mask: 1.4916, decode.d2.loss_dice: 3.1733, decode.d3.loss_cls: 0.1403, decode.d3.loss_mask: 1.4994, decode.d3.loss_dice: 3.1608, decode.d4.loss_cls: 0.1224, decode.d4.loss_mask: 1.5052, decode.d4.loss_dice: 3.1627, decode.d5.loss_cls: 0.1274, decode.d5.loss_mask: 1.5030, decode.d5.loss_dice: 3.1731, decode.d6.loss_cls: 0.1191, decode.d6.loss_mask: 1.5040, decode.d6.loss_dice: 3.1620, decode.d7.loss_cls: 0.1271, decode.d7.loss_mask: 1.5064, decode.d7.loss_dice: 3.1791, decode.d8.loss_cls: 0.1111, decode.d8.loss_mask: 1.5070, decode.d8.loss_dice: 3.1690, loss: 53.6943
2023-03-17 20:22:42,145 - mmseg - INFO - Iter [950/4000]	lr: 7.432e-06, eta: 0:29:24, time: 0.551, data_time: 0.005, memory: 14565, decode.loss_cls: 0.1228, decode.loss_mask: 1.5127, decode.loss_dice: 3.1467, decode.d0.loss_cls: 5.5478, decode.d0.loss_mask: 1.4923, decode.d0.loss_dice: 3.1114, decode.d1.loss_cls: 0.3649, decode.d1.loss_mask: 1.5019, decode.d1.loss_dice: 3.1428, decode.d2.loss_cls: 0.1849, decode.d2.loss_mask: 1.5096, decode.d2.loss_dice: 3.1667, decode.d3.loss_cls: 0.1409, decode.d3.loss_mask: 1.5101, decode.d3.loss_dice: 3.1704, decode.d4.loss_cls: 0.1335, decode.d4.loss_mask: 1.5101, decode.d4.loss_dice: 3.1396, decode.d5.loss_cls: 0.1225, decode.d5.loss_mask: 1.5123, decode.d5.loss_dice: 3.1704, decode.d6.loss_cls: 0.1268, decode.d6.loss_mask: 1.5094, decode.d6.loss_dice: 3.1517, decode.d7.loss_cls: 0.1236, decode.d7.loss_mask: 1.5169, decode.d7.loss_dice: 3.1508, decode.d8.loss_cls: 0.1211, decode.d8.loss_mask: 1.5101, decode.d8.loss_dice: 3.1576, loss: 53.5825
2023-03-17 20:23:10,633 - mmseg - INFO - Saving checkpoint at 1000 iterations
2023-03-17 20:23:16,336 - mmseg - INFO - Exp name: mask2former_beit_adapter_base_480_40k_pascal_context_59_ss.py
2023-03-17 20:23:16,341 - mmseg - INFO - Iter [1000/4000]	lr: 7.695e-06, eta: 0:29:11, time: 0.684, data_time: 0.007, memory: 14565, decode.loss_cls: 0.0587, decode.loss_mask: 1.4628, decode.loss_dice: 3.1528, decode.d0.loss_cls: 5.4526, decode.d0.loss_mask: 1.4593, decode.d0.loss_dice: 3.1644, decode.d1.loss_cls: 0.2797, decode.d1.loss_mask: 1.4582, decode.d1.loss_dice: 3.1929, decode.d2.loss_cls: 0.1418, decode.d2.loss_mask: 1.4578, decode.d2.loss_dice: 3.1476, decode.d3.loss_cls: 0.0910, decode.d3.loss_mask: 1.4656, decode.d3.loss_dice: 3.1757, decode.d4.loss_cls: 0.0836, decode.d4.loss_mask: 1.4654, decode.d4.loss_dice: 3.1648, decode.d5.loss_cls: 0.0837, decode.d5.loss_mask: 1.4601, decode.d5.loss_dice: 3.1654, decode.d6.loss_cls: 0.0602, decode.d6.loss_mask: 1.4614, decode.d6.loss_dice: 3.1883, decode.d7.loss_cls: 0.0711, decode.d7.loss_mask: 1.4644, decode.d7.loss_dice: 3.1652, decode.d8.loss_cls: 0.0814, decode.d8.loss_mask: 1.4622, decode.d8.loss_dice: 3.1803, loss: 52.7183
2023-03-17 20:23:44,275 - mmseg - INFO - Iter [1050/4000]	lr: 7.946e-06, eta: 0:28:38, time: 0.559, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0721, decode.loss_mask: 1.4195, decode.loss_dice: 3.1542, decode.d0.loss_cls: 5.4211, decode.d0.loss_mask: 1.4115, decode.d0.loss_dice: 3.1408, decode.d1.loss_cls: 0.2660, decode.d1.loss_mask: 1.4197, decode.d1.loss_dice: 3.1498, decode.d2.loss_cls: 0.1376, decode.d2.loss_mask: 1.4216, decode.d2.loss_dice: 3.1626, decode.d3.loss_cls: 0.0964, decode.d3.loss_mask: 1.4188, decode.d3.loss_dice: 3.1527, decode.d4.loss_cls: 0.0770, decode.d4.loss_mask: 1.4176, decode.d4.loss_dice: 3.1606, decode.d5.loss_cls: 0.0809, decode.d5.loss_mask: 1.4178, decode.d5.loss_dice: 3.1708, decode.d6.loss_cls: 0.0806, decode.d6.loss_mask: 1.4194, decode.d6.loss_dice: 3.1859, decode.d7.loss_cls: 0.0671, decode.d7.loss_mask: 1.4229, decode.d7.loss_dice: 3.1456, decode.d8.loss_cls: 0.0793, decode.d8.loss_mask: 1.4250, decode.d8.loss_dice: 3.1574, loss: 52.1521
2023-03-17 20:24:13,149 - mmseg - INFO - Iter [1100/4000]	lr: 8.183e-06, eta: 0:28:09, time: 0.577, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0994, decode.loss_mask: 1.4710, decode.loss_dice: 3.1347, decode.d0.loss_cls: 5.3940, decode.d0.loss_mask: 1.4726, decode.d0.loss_dice: 3.1134, decode.d1.loss_cls: 0.2864, decode.d1.loss_mask: 1.4666, decode.d1.loss_dice: 3.1684, decode.d2.loss_cls: 0.1699, decode.d2.loss_mask: 1.4749, decode.d2.loss_dice: 3.1512, decode.d3.loss_cls: 0.1131, decode.d3.loss_mask: 1.4779, decode.d3.loss_dice: 3.1352, decode.d4.loss_cls: 0.1296, decode.d4.loss_mask: 1.4688, decode.d4.loss_dice: 3.1332, decode.d5.loss_cls: 0.0945, decode.d5.loss_mask: 1.4741, decode.d5.loss_dice: 3.1446, decode.d6.loss_cls: 0.1018, decode.d6.loss_mask: 1.4710, decode.d6.loss_dice: 3.1228, decode.d7.loss_cls: 0.0861, decode.d7.loss_mask: 1.4712, decode.d7.loss_dice: 3.1581, decode.d8.loss_cls: 0.0962, decode.d8.loss_mask: 1.4751, decode.d8.loss_dice: 3.1400, loss: 52.6960
2023-03-17 20:24:40,792 - mmseg - INFO - Iter [1150/4000]	lr: 8.408e-06, eta: 0:27:36, time: 0.553, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0643, decode.loss_mask: 1.5096, decode.loss_dice: 3.1777, decode.d0.loss_cls: 5.3492, decode.d0.loss_mask: 1.5002, decode.d0.loss_dice: 3.1146, decode.d1.loss_cls: 0.2136, decode.d1.loss_mask: 1.5154, decode.d1.loss_dice: 3.1891, decode.d2.loss_cls: 0.1157, decode.d2.loss_mask: 1.5054, decode.d2.loss_dice: 3.1686, decode.d3.loss_cls: 0.0844, decode.d3.loss_mask: 1.5121, decode.d3.loss_dice: 3.1239, decode.d4.loss_cls: 0.0771, decode.d4.loss_mask: 1.5127, decode.d4.loss_dice: 3.1422, decode.d5.loss_cls: 0.0747, decode.d5.loss_mask: 1.5082, decode.d5.loss_dice: 3.1516, decode.d6.loss_cls: 0.0631, decode.d6.loss_mask: 1.5142, decode.d6.loss_dice: 3.1640, decode.d7.loss_cls: 0.0569, decode.d7.loss_mask: 1.5155, decode.d7.loss_dice: 3.1495, decode.d8.loss_cls: 0.0586, decode.d8.loss_mask: 1.5093, decode.d8.loss_dice: 3.1825, loss: 52.8240
2023-03-17 20:25:08,283 - mmseg - INFO - Iter [1200/4000]	lr: 8.620e-06, eta: 0:27:03, time: 0.550, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0580, decode.loss_mask: 1.4665, decode.loss_dice: 3.1092, decode.d0.loss_cls: 5.3564, decode.d0.loss_mask: 1.4657, decode.d0.loss_dice: 3.0893, decode.d1.loss_cls: 0.2236, decode.d1.loss_mask: 1.4613, decode.d1.loss_dice: 3.1095, decode.d2.loss_cls: 0.1063, decode.d2.loss_mask: 1.4658, decode.d2.loss_dice: 3.1041, decode.d3.loss_cls: 0.0749, decode.d3.loss_mask: 1.4654, decode.d3.loss_dice: 3.0942, decode.d4.loss_cls: 0.0795, decode.d4.loss_mask: 1.4685, decode.d4.loss_dice: 3.0839, decode.d5.loss_cls: 0.0647, decode.d5.loss_mask: 1.4657, decode.d5.loss_dice: 3.1049, decode.d6.loss_cls: 0.0591, decode.d6.loss_mask: 1.4632, decode.d6.loss_dice: 3.0921, decode.d7.loss_cls: 0.0762, decode.d7.loss_mask: 1.4649, decode.d7.loss_dice: 3.1168, decode.d8.loss_cls: 0.0624, decode.d8.loss_mask: 1.4622, decode.d8.loss_dice: 3.0969, loss: 51.8115
2023-03-17 20:25:37,124 - mmseg - INFO - Iter [1250/4000]	lr: 8.819e-06, eta: 0:26:34, time: 0.577, data_time: 0.048, memory: 14565, decode.loss_cls: 0.0836, decode.loss_mask: 1.4701, decode.loss_dice: 3.0990, decode.d0.loss_cls: 5.2431, decode.d0.loss_mask: 1.4749, decode.d0.loss_dice: 3.0884, decode.d1.loss_cls: 0.2357, decode.d1.loss_mask: 1.4699, decode.d1.loss_dice: 3.1168, decode.d2.loss_cls: 0.1273, decode.d2.loss_mask: 1.4672, decode.d2.loss_dice: 3.1003, decode.d3.loss_cls: 0.0918, decode.d3.loss_mask: 1.4715, decode.d3.loss_dice: 3.0969, decode.d4.loss_cls: 0.1044, decode.d4.loss_mask: 1.4661, decode.d4.loss_dice: 3.0952, decode.d5.loss_cls: 0.0863, decode.d5.loss_mask: 1.4768, decode.d5.loss_dice: 3.1141, decode.d6.loss_cls: 0.0969, decode.d6.loss_mask: 1.4674, decode.d6.loss_dice: 3.1051, decode.d7.loss_cls: 0.0897, decode.d7.loss_mask: 1.4734, decode.d7.loss_dice: 3.1087, decode.d8.loss_cls: 0.0891, decode.d8.loss_mask: 1.4720, decode.d8.loss_dice: 3.1044, loss: 51.9860
2023-03-17 20:26:04,015 - mmseg - INFO - Iter [1300/4000]	lr: 9.006e-06, eta: 0:26:00, time: 0.538, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0727, decode.loss_mask: 1.4553, decode.loss_dice: 3.1381, decode.d0.loss_cls: 5.2193, decode.d0.loss_mask: 1.4426, decode.d0.loss_dice: 3.1165, decode.d1.loss_cls: 0.1816, decode.d1.loss_mask: 1.4440, decode.d1.loss_dice: 3.1219, decode.d2.loss_cls: 0.1011, decode.d2.loss_mask: 1.4464, decode.d2.loss_dice: 3.1176, decode.d3.loss_cls: 0.0716, decode.d3.loss_mask: 1.4523, decode.d3.loss_dice: 3.1158, decode.d4.loss_cls: 0.0678, decode.d4.loss_mask: 1.4564, decode.d4.loss_dice: 3.1345, decode.d5.loss_cls: 0.0678, decode.d5.loss_mask: 1.4483, decode.d5.loss_dice: 3.1260, decode.d6.loss_cls: 0.0698, decode.d6.loss_mask: 1.4504, decode.d6.loss_dice: 3.1080, decode.d7.loss_cls: 0.0729, decode.d7.loss_mask: 1.4540, decode.d7.loss_dice: 3.1201, decode.d8.loss_cls: 0.0722, decode.d8.loss_mask: 1.4548, decode.d8.loss_dice: 3.1128, loss: 51.7126
2023-03-17 20:26:31,512 - mmseg - INFO - Iter [1350/4000]	lr: 9.179e-06, eta: 0:25:29, time: 0.550, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0725, decode.loss_mask: 1.4898, decode.loss_dice: 3.1312, decode.d0.loss_cls: 5.2298, decode.d0.loss_mask: 1.4865, decode.d0.loss_dice: 3.1085, decode.d1.loss_cls: 0.1574, decode.d1.loss_mask: 1.4959, decode.d1.loss_dice: 3.1487, decode.d2.loss_cls: 0.0902, decode.d2.loss_mask: 1.4916, decode.d2.loss_dice: 3.1314, decode.d3.loss_cls: 0.0725, decode.d3.loss_mask: 1.4902, decode.d3.loss_dice: 3.1221, decode.d4.loss_cls: 0.0688, decode.d4.loss_mask: 1.4917, decode.d4.loss_dice: 3.1390, decode.d5.loss_cls: 0.0697, decode.d5.loss_mask: 1.4900, decode.d5.loss_dice: 3.1318, decode.d6.loss_cls: 0.0573, decode.d6.loss_mask: 1.4891, decode.d6.loss_dice: 3.1288, decode.d7.loss_cls: 0.0687, decode.d7.loss_mask: 1.4942, decode.d7.loss_dice: 3.1476, decode.d8.loss_cls: 0.0712, decode.d8.loss_mask: 1.4915, decode.d8.loss_dice: 3.1301, loss: 52.1878
2023-03-17 20:26:58,556 - mmseg - INFO - Iter [1400/4000]	lr: 9.340e-06, eta: 0:24:57, time: 0.541, data_time: 0.005, memory: 14565, decode.loss_cls: 0.1081, decode.loss_mask: 1.4638, decode.loss_dice: 3.1368, decode.d0.loss_cls: 5.1795, decode.d0.loss_mask: 1.4389, decode.d0.loss_dice: 3.1053, decode.d1.loss_cls: 0.2040, decode.d1.loss_mask: 1.4699, decode.d1.loss_dice: 3.1422, decode.d2.loss_cls: 0.1505, decode.d2.loss_mask: 1.4618, decode.d2.loss_dice: 3.0961, decode.d3.loss_cls: 0.1125, decode.d3.loss_mask: 1.4588, decode.d3.loss_dice: 3.1132, decode.d4.loss_cls: 0.1006, decode.d4.loss_mask: 1.4554, decode.d4.loss_dice: 3.1020, decode.d5.loss_cls: 0.0999, decode.d5.loss_mask: 1.4547, decode.d5.loss_dice: 3.1252, decode.d6.loss_cls: 0.1033, decode.d6.loss_mask: 1.4581, decode.d6.loss_dice: 3.1159, decode.d7.loss_cls: 0.1057, decode.d7.loss_mask: 1.4655, decode.d7.loss_dice: 3.1223, decode.d8.loss_cls: 0.1045, decode.d8.loss_mask: 1.4609, decode.d8.loss_dice: 3.1367, loss: 52.0520
2023-03-17 20:27:25,582 - mmseg - INFO - Iter [1450/4000]	lr: 9.488e-06, eta: 0:24:25, time: 0.541, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0650, decode.loss_mask: 1.4706, decode.loss_dice: 3.1437, decode.d0.loss_cls: 5.0960, decode.d0.loss_mask: 1.4513, decode.d0.loss_dice: 3.0999, decode.d1.loss_cls: 0.1572, decode.d1.loss_mask: 1.4623, decode.d1.loss_dice: 3.1315, decode.d2.loss_cls: 0.0942, decode.d2.loss_mask: 1.4647, decode.d2.loss_dice: 3.1406, decode.d3.loss_cls: 0.0765, decode.d3.loss_mask: 1.4662, decode.d3.loss_dice: 3.1140, decode.d4.loss_cls: 0.0807, decode.d4.loss_mask: 1.4671, decode.d4.loss_dice: 3.1545, decode.d5.loss_cls: 0.0783, decode.d5.loss_mask: 1.4733, decode.d5.loss_dice: 3.1142, decode.d6.loss_cls: 0.0643, decode.d6.loss_mask: 1.4689, decode.d6.loss_dice: 3.1505, decode.d7.loss_cls: 0.0673, decode.d7.loss_mask: 1.4692, decode.d7.loss_dice: 3.1422, decode.d8.loss_cls: 0.0699, decode.d8.loss_mask: 1.4686, decode.d8.loss_dice: 3.1218, loss: 51.8245
2023-03-17 20:27:52,930 - mmseg - INFO - Iter [1500/4000]	lr: 9.623e-06, eta: 0:23:54, time: 0.547, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0519, decode.loss_mask: 1.4898, decode.loss_dice: 3.1388, decode.d0.loss_cls: 4.9899, decode.d0.loss_mask: 1.4871, decode.d0.loss_dice: 3.0983, decode.d1.loss_cls: 0.1065, decode.d1.loss_mask: 1.4996, decode.d1.loss_dice: 3.1440, decode.d2.loss_cls: 0.0717, decode.d2.loss_mask: 1.4903, decode.d2.loss_dice: 3.1374, decode.d3.loss_cls: 0.0556, decode.d3.loss_mask: 1.4922, decode.d3.loss_dice: 3.1322, decode.d4.loss_cls: 0.0469, decode.d4.loss_mask: 1.4883, decode.d4.loss_dice: 3.1308, decode.d5.loss_cls: 0.0459, decode.d5.loss_mask: 1.4894, decode.d5.loss_dice: 3.1504, decode.d6.loss_cls: 0.0530, decode.d6.loss_mask: 1.4866, decode.d6.loss_dice: 3.1533, decode.d7.loss_cls: 0.0519, decode.d7.loss_mask: 1.4921, decode.d7.loss_dice: 3.1397, decode.d8.loss_cls: 0.0586, decode.d8.loss_mask: 1.4912, decode.d8.loss_dice: 3.1474, loss: 51.8108
2023-03-17 20:28:21,500 - mmseg - INFO - Iter [1550/4000]	lr: 9.437e-06, eta: 0:23:25, time: 0.571, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0663, decode.loss_mask: 1.4620, decode.loss_dice: 3.1024, decode.d0.loss_cls: 4.9410, decode.d0.loss_mask: 1.4687, decode.d0.loss_dice: 3.0773, decode.d1.loss_cls: 0.1308, decode.d1.loss_mask: 1.4612, decode.d1.loss_dice: 3.0951, decode.d2.loss_cls: 0.0832, decode.d2.loss_mask: 1.4593, decode.d2.loss_dice: 3.0908, decode.d3.loss_cls: 0.0718, decode.d3.loss_mask: 1.4586, decode.d3.loss_dice: 3.0938, decode.d4.loss_cls: 0.0714, decode.d4.loss_mask: 1.4560, decode.d4.loss_dice: 3.0877, decode.d5.loss_cls: 0.0660, decode.d5.loss_mask: 1.4646, decode.d5.loss_dice: 3.0955, decode.d6.loss_cls: 0.0659, decode.d6.loss_mask: 1.4607, decode.d6.loss_dice: 3.0815, decode.d7.loss_cls: 0.0682, decode.d7.loss_mask: 1.4603, decode.d7.loss_dice: 3.0728, decode.d8.loss_cls: 0.0614, decode.d8.loss_mask: 1.4645, decode.d8.loss_dice: 3.0747, loss: 51.1135
2023-03-17 20:28:48,822 - mmseg - INFO - Iter [1600/4000]	lr: 9.244e-06, eta: 0:22:54, time: 0.546, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0804, decode.loss_mask: 1.4356, decode.loss_dice: 3.1220, decode.d0.loss_cls: 4.8760, decode.d0.loss_mask: 1.4412, decode.d0.loss_dice: 3.0829, decode.d1.loss_cls: 0.1499, decode.d1.loss_mask: 1.4470, decode.d1.loss_dice: 3.1251, decode.d2.loss_cls: 0.1007, decode.d2.loss_mask: 1.4474, decode.d2.loss_dice: 3.1386, decode.d3.loss_cls: 0.0881, decode.d3.loss_mask: 1.4466, decode.d3.loss_dice: 3.1011, decode.d4.loss_cls: 0.0883, decode.d4.loss_mask: 1.4394, decode.d4.loss_dice: 3.1213, decode.d5.loss_cls: 0.0832, decode.d5.loss_mask: 1.4416, decode.d5.loss_dice: 3.1355, decode.d6.loss_cls: 0.0844, decode.d6.loss_mask: 1.4415, decode.d6.loss_dice: 3.1419, decode.d7.loss_cls: 0.0904, decode.d7.loss_mask: 1.4358, decode.d7.loss_dice: 3.1016, decode.d8.loss_cls: 0.0838, decode.d8.loss_mask: 1.4373, decode.d8.loss_dice: 3.1181, loss: 51.3265
2023-03-17 20:29:15,750 - mmseg - INFO - Iter [1650/4000]	lr: 9.052e-06, eta: 0:22:23, time: 0.539, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0972, decode.loss_mask: 1.3871, decode.loss_dice: 3.0991, decode.d0.loss_cls: 4.8554, decode.d0.loss_mask: 1.3763, decode.d0.loss_dice: 3.0544, decode.d1.loss_cls: 0.1415, decode.d1.loss_mask: 1.3884, decode.d1.loss_dice: 3.0882, decode.d2.loss_cls: 0.1001, decode.d2.loss_mask: 1.3949, decode.d2.loss_dice: 3.0796, decode.d3.loss_cls: 0.0848, decode.d3.loss_mask: 1.3901, decode.d3.loss_dice: 3.0761, decode.d4.loss_cls: 0.0846, decode.d4.loss_mask: 1.3921, decode.d4.loss_dice: 3.1020, decode.d5.loss_cls: 0.0966, decode.d5.loss_mask: 1.3862, decode.d5.loss_dice: 3.0819, decode.d6.loss_cls: 0.0841, decode.d6.loss_mask: 1.3925, decode.d6.loss_dice: 3.1013, decode.d7.loss_cls: 0.0803, decode.d7.loss_mask: 1.3932, decode.d7.loss_dice: 3.1101, decode.d8.loss_cls: 0.0816, decode.d8.loss_mask: 1.3920, decode.d8.loss_dice: 3.1096, loss: 50.5010
2023-03-17 20:29:44,561 - mmseg - INFO - Iter [1700/4000]	lr: 8.859e-06, eta: 0:21:55, time: 0.576, data_time: 0.049, memory: 14565, decode.loss_cls: 0.0794, decode.loss_mask: 1.3732, decode.loss_dice: 3.0891, decode.d0.loss_cls: 4.7411, decode.d0.loss_mask: 1.3721, decode.d0.loss_dice: 3.0459, decode.d1.loss_cls: 0.1152, decode.d1.loss_mask: 1.3740, decode.d1.loss_dice: 3.0936, decode.d2.loss_cls: 0.0877, decode.d2.loss_mask: 1.3699, decode.d2.loss_dice: 3.0809, decode.d3.loss_cls: 0.0804, decode.d3.loss_mask: 1.3716, decode.d3.loss_dice: 3.0447, decode.d4.loss_cls: 0.0672, decode.d4.loss_mask: 1.3685, decode.d4.loss_dice: 3.0627, decode.d5.loss_cls: 0.0773, decode.d5.loss_mask: 1.3721, decode.d5.loss_dice: 3.0784, decode.d6.loss_cls: 0.0730, decode.d6.loss_mask: 1.3711, decode.d6.loss_dice: 3.0867, decode.d7.loss_cls: 0.0740, decode.d7.loss_mask: 1.3707, decode.d7.loss_dice: 3.0719, decode.d8.loss_cls: 0.0732, decode.d8.loss_mask: 1.3683, decode.d8.loss_dice: 3.0745, loss: 49.9085
2023-03-17 20:30:11,859 - mmseg - INFO - Iter [1750/4000]	lr: 8.666e-06, eta: 0:21:24, time: 0.546, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0556, decode.loss_mask: 1.4564, decode.loss_dice: 3.0976, decode.d0.loss_cls: 4.7350, decode.d0.loss_mask: 1.4484, decode.d0.loss_dice: 3.0589, decode.d1.loss_cls: 0.1169, decode.d1.loss_mask: 1.4516, decode.d1.loss_dice: 3.1077, decode.d2.loss_cls: 0.0743, decode.d2.loss_mask: 1.4518, decode.d2.loss_dice: 3.0821, decode.d3.loss_cls: 0.0574, decode.d3.loss_mask: 1.4496, decode.d3.loss_dice: 3.0796, decode.d4.loss_cls: 0.0649, decode.d4.loss_mask: 1.4509, decode.d4.loss_dice: 3.0931, decode.d5.loss_cls: 0.0680, decode.d5.loss_mask: 1.4521, decode.d5.loss_dice: 3.0886, decode.d6.loss_cls: 0.0641, decode.d6.loss_mask: 1.4517, decode.d6.loss_dice: 3.0669, decode.d7.loss_cls: 0.0596, decode.d7.loss_mask: 1.4550, decode.d7.loss_dice: 3.0984, decode.d8.loss_cls: 0.0615, decode.d8.loss_mask: 1.4549, decode.d8.loss_dice: 3.0633, loss: 50.7160
2023-03-17 20:30:39,216 - mmseg - INFO - Iter [1800/4000]	lr: 8.474e-06, eta: 0:20:54, time: 0.547, data_time: 0.006, memory: 14565, decode.loss_cls: 0.1215, decode.loss_mask: 1.4490, decode.loss_dice: 3.1165, decode.d0.loss_cls: 4.7051, decode.d0.loss_mask: 1.4231, decode.d0.loss_dice: 3.0938, decode.d1.loss_cls: 0.1568, decode.d1.loss_mask: 1.4521, decode.d1.loss_dice: 3.1467, decode.d2.loss_cls: 0.1359, decode.d2.loss_mask: 1.4506, decode.d2.loss_dice: 3.1224, decode.d3.loss_cls: 0.1061, decode.d3.loss_mask: 1.4563, decode.d3.loss_dice: 3.1295, decode.d4.loss_cls: 0.1159, decode.d4.loss_mask: 1.4507, decode.d4.loss_dice: 3.1267, decode.d5.loss_cls: 0.1061, decode.d5.loss_mask: 1.4464, decode.d5.loss_dice: 3.1511, decode.d6.loss_cls: 0.1094, decode.d6.loss_mask: 1.4486, decode.d6.loss_dice: 3.1329, decode.d7.loss_cls: 0.1008, decode.d7.loss_mask: 1.4521, decode.d7.loss_dice: 3.1224, decode.d8.loss_cls: 0.1045, decode.d8.loss_mask: 1.4508, decode.d8.loss_dice: 3.1056, loss: 51.4893
2023-03-17 20:31:06,052 - mmseg - INFO - Iter [1850/4000]	lr: 8.281e-06, eta: 0:20:24, time: 0.537, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0628, decode.loss_mask: 1.4522, decode.loss_dice: 3.1037, decode.d0.loss_cls: 4.6114, decode.d0.loss_mask: 1.4383, decode.d0.loss_dice: 3.0211, decode.d1.loss_cls: 0.0992, decode.d1.loss_mask: 1.4552, decode.d1.loss_dice: 3.0981, decode.d2.loss_cls: 0.0791, decode.d2.loss_mask: 1.4491, decode.d2.loss_dice: 3.0882, decode.d3.loss_cls: 0.0711, decode.d3.loss_mask: 1.4513, decode.d3.loss_dice: 3.0930, decode.d4.loss_cls: 0.0650, decode.d4.loss_mask: 1.4516, decode.d4.loss_dice: 3.0858, decode.d5.loss_cls: 0.0632, decode.d5.loss_mask: 1.4520, decode.d5.loss_dice: 3.0881, decode.d6.loss_cls: 0.0767, decode.d6.loss_mask: 1.4488, decode.d6.loss_dice: 3.0811, decode.d7.loss_cls: 0.0668, decode.d7.loss_mask: 1.4491, decode.d7.loss_dice: 3.0937, decode.d8.loss_cls: 0.0670, decode.d8.loss_mask: 1.4485, decode.d8.loss_dice: 3.0821, loss: 50.5935
2023-03-17 20:31:32,794 - mmseg - INFO - Iter [1900/4000]	lr: 8.089e-06, eta: 0:19:54, time: 0.535, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0543, decode.loss_mask: 1.4285, decode.loss_dice: 3.0874, decode.d0.loss_cls: 4.5974, decode.d0.loss_mask: 1.4186, decode.d0.loss_dice: 3.0513, decode.d1.loss_cls: 0.0952, decode.d1.loss_mask: 1.4310, decode.d1.loss_dice: 3.1019, decode.d2.loss_cls: 0.0706, decode.d2.loss_mask: 1.4291, decode.d2.loss_dice: 3.0961, decode.d3.loss_cls: 0.0485, decode.d3.loss_mask: 1.4298, decode.d3.loss_dice: 3.0915, decode.d4.loss_cls: 0.0578, decode.d4.loss_mask: 1.4318, decode.d4.loss_dice: 3.0876, decode.d5.loss_cls: 0.0542, decode.d5.loss_mask: 1.4306, decode.d5.loss_dice: 3.0832, decode.d6.loss_cls: 0.0565, decode.d6.loss_mask: 1.4285, decode.d6.loss_dice: 3.0916, decode.d7.loss_cls: 0.0534, decode.d7.loss_mask: 1.4283, decode.d7.loss_dice: 3.1034, decode.d8.loss_cls: 0.0531, decode.d8.loss_mask: 1.4288, decode.d8.loss_dice: 3.0897, loss: 50.3096
2023-03-17 20:31:59,910 - mmseg - INFO - Iter [1950/4000]	lr: 7.896e-06, eta: 0:19:24, time: 0.542, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0529, decode.loss_mask: 1.4499, decode.loss_dice: 3.0782, decode.d0.loss_cls: 4.5480, decode.d0.loss_mask: 1.4474, decode.d0.loss_dice: 3.0344, decode.d1.loss_cls: 0.1031, decode.d1.loss_mask: 1.4502, decode.d1.loss_dice: 3.1079, decode.d2.loss_cls: 0.0676, decode.d2.loss_mask: 1.4430, decode.d2.loss_dice: 3.0858, decode.d3.loss_cls: 0.0586, decode.d3.loss_mask: 1.4457, decode.d3.loss_dice: 3.0960, decode.d4.loss_cls: 0.0565, decode.d4.loss_mask: 1.4495, decode.d4.loss_dice: 3.0753, decode.d5.loss_cls: 0.0564, decode.d5.loss_mask: 1.4495, decode.d5.loss_dice: 3.0886, decode.d6.loss_cls: 0.0555, decode.d6.loss_mask: 1.4476, decode.d6.loss_dice: 3.0707, decode.d7.loss_cls: 0.0551, decode.d7.loss_mask: 1.4444, decode.d7.loss_dice: 3.0733, decode.d8.loss_cls: 0.0545, decode.d8.loss_mask: 1.4465, decode.d8.loss_dice: 3.0809, loss: 50.3729
2023-03-17 20:32:27,626 - mmseg - INFO - Saving checkpoint at 2000 iterations
2023-03-17 20:32:33,182 - mmseg - INFO - Exp name: mask2former_beit_adapter_base_480_40k_pascal_context_59_ss.py
2023-03-17 20:32:33,184 - mmseg - INFO - Iter [2000/4000]	lr: 7.704e-06, eta: 0:19:00, time: 0.665, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0459, decode.loss_mask: 1.4136, decode.loss_dice: 3.1065, decode.d0.loss_cls: 4.4320, decode.d0.loss_mask: 1.4032, decode.d0.loss_dice: 3.1013, decode.d1.loss_cls: 0.0862, decode.d1.loss_mask: 1.4140, decode.d1.loss_dice: 3.1353, decode.d2.loss_cls: 0.0566, decode.d2.loss_mask: 1.4149, decode.d2.loss_dice: 3.1190, decode.d3.loss_cls: 0.0442, decode.d3.loss_mask: 1.4160, decode.d3.loss_dice: 3.1004, decode.d4.loss_cls: 0.0458, decode.d4.loss_mask: 1.4097, decode.d4.loss_dice: 3.1257, decode.d5.loss_cls: 0.0456, decode.d5.loss_mask: 1.4158, decode.d5.loss_dice: 3.1357, decode.d6.loss_cls: 0.0468, decode.d6.loss_mask: 1.4117, decode.d6.loss_dice: 3.1114, decode.d7.loss_cls: 0.0458, decode.d7.loss_mask: 1.4147, decode.d7.loss_dice: 3.1335, decode.d8.loss_cls: 0.0463, decode.d8.loss_mask: 1.4138, decode.d8.loss_dice: 3.1280, loss: 50.2194
2023-03-17 20:33:00,217 - mmseg - INFO - Iter [2050/4000]	lr: 7.511e-06, eta: 0:18:30, time: 0.541, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0687, decode.loss_mask: 1.4373, decode.loss_dice: 3.0760, decode.d0.loss_cls: 4.3855, decode.d0.loss_mask: 1.4388, decode.d0.loss_dice: 3.0455, decode.d1.loss_cls: 0.0964, decode.d1.loss_mask: 1.4436, decode.d1.loss_dice: 3.0787, decode.d2.loss_cls: 0.0813, decode.d2.loss_mask: 1.4380, decode.d2.loss_dice: 3.0926, decode.d3.loss_cls: 0.0596, decode.d3.loss_mask: 1.4411, decode.d3.loss_dice: 3.0522, decode.d4.loss_cls: 0.0678, decode.d4.loss_mask: 1.4396, decode.d4.loss_dice: 3.0602, decode.d5.loss_cls: 0.0617, decode.d5.loss_mask: 1.4422, decode.d5.loss_dice: 3.0862, decode.d6.loss_cls: 0.0752, decode.d6.loss_mask: 1.4382, decode.d6.loss_dice: 3.0844, decode.d7.loss_cls: 0.0682, decode.d7.loss_mask: 1.4394, decode.d7.loss_dice: 3.0625, decode.d8.loss_cls: 0.0659, decode.d8.loss_mask: 1.4375, decode.d8.loss_dice: 3.0676, loss: 50.1318
2023-03-17 20:33:30,876 - mmseg - INFO - Iter [2100/4000]	lr: 7.319e-06, eta: 0:18:04, time: 0.613, data_time: 0.048, memory: 14565, decode.loss_cls: 0.1031, decode.loss_mask: 1.4282, decode.loss_dice: 3.0806, decode.d0.loss_cls: 4.3189, decode.d0.loss_mask: 1.4075, decode.d0.loss_dice: 3.0153, decode.d1.loss_cls: 0.1449, decode.d1.loss_mask: 1.4305, decode.d1.loss_dice: 3.0711, decode.d2.loss_cls: 0.1202, decode.d2.loss_mask: 1.4230, decode.d2.loss_dice: 3.0873, decode.d3.loss_cls: 0.1113, decode.d3.loss_mask: 1.4301, decode.d3.loss_dice: 3.0688, decode.d4.loss_cls: 0.1000, decode.d4.loss_mask: 1.4294, decode.d4.loss_dice: 3.0667, decode.d5.loss_cls: 0.1039, decode.d5.loss_mask: 1.4208, decode.d5.loss_dice: 3.0939, decode.d6.loss_cls: 0.1165, decode.d6.loss_mask: 1.4232, decode.d6.loss_dice: 3.0701, decode.d7.loss_cls: 0.0988, decode.d7.loss_mask: 1.4275, decode.d7.loss_dice: 3.0790, decode.d8.loss_cls: 0.0893, decode.d8.loss_mask: 1.4295, decode.d8.loss_dice: 3.0692, loss: 50.2588
2023-03-17 20:33:58,343 - mmseg - INFO - Iter [2150/4000]	lr: 7.126e-06, eta: 0:17:34, time: 0.549, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0557, decode.loss_mask: 1.4587, decode.loss_dice: 3.0799, decode.d0.loss_cls: 4.2971, decode.d0.loss_mask: 1.4411, decode.d0.loss_dice: 3.0453, decode.d1.loss_cls: 0.1047, decode.d1.loss_mask: 1.4567, decode.d1.loss_dice: 3.0559, decode.d2.loss_cls: 0.0597, decode.d2.loss_mask: 1.4553, decode.d2.loss_dice: 3.0874, decode.d3.loss_cls: 0.0582, decode.d3.loss_mask: 1.4554, decode.d3.loss_dice: 3.0725, decode.d4.loss_cls: 0.0609, decode.d4.loss_mask: 1.4539, decode.d4.loss_dice: 3.0520, decode.d5.loss_cls: 0.0526, decode.d5.loss_mask: 1.4580, decode.d5.loss_dice: 3.0577, decode.d6.loss_cls: 0.0643, decode.d6.loss_mask: 1.4532, decode.d6.loss_dice: 3.0663, decode.d7.loss_cls: 0.0644, decode.d7.loss_mask: 1.4520, decode.d7.loss_dice: 3.0670, decode.d8.loss_cls: 0.0565, decode.d8.loss_mask: 1.4532, decode.d8.loss_dice: 3.0732, loss: 50.0687
2023-03-17 20:34:25,988 - mmseg - INFO - Iter [2200/4000]	lr: 6.934e-06, eta: 0:17:05, time: 0.553, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0600, decode.loss_mask: 1.4576, decode.loss_dice: 3.0767, decode.d0.loss_cls: 4.2405, decode.d0.loss_mask: 1.4492, decode.d0.loss_dice: 3.0596, decode.d1.loss_cls: 0.0950, decode.d1.loss_mask: 1.4518, decode.d1.loss_dice: 3.0925, decode.d2.loss_cls: 0.0824, decode.d2.loss_mask: 1.4520, decode.d2.loss_dice: 3.0725, decode.d3.loss_cls: 0.0788, decode.d3.loss_mask: 1.4520, decode.d3.loss_dice: 3.0487, decode.d4.loss_cls: 0.0716, decode.d4.loss_mask: 1.4536, decode.d4.loss_dice: 3.0860, decode.d5.loss_cls: 0.0650, decode.d5.loss_mask: 1.4530, decode.d5.loss_dice: 3.0585, decode.d6.loss_cls: 0.0603, decode.d6.loss_mask: 1.4527, decode.d6.loss_dice: 3.0788, decode.d7.loss_cls: 0.0618, decode.d7.loss_mask: 1.4522, decode.d7.loss_dice: 3.0655, decode.d8.loss_cls: 0.0630, decode.d8.loss_mask: 1.4546, decode.d8.loss_dice: 3.0868, loss: 50.1324
2023-03-17 20:34:53,227 - mmseg - INFO - Iter [2250/4000]	lr: 6.741e-06, eta: 0:16:36, time: 0.545, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0700, decode.loss_mask: 1.4477, decode.loss_dice: 3.0846, decode.d0.loss_cls: 4.1765, decode.d0.loss_mask: 1.4210, decode.d0.loss_dice: 3.0842, decode.d1.loss_cls: 0.0895, decode.d1.loss_mask: 1.4461, decode.d1.loss_dice: 3.1203, decode.d2.loss_cls: 0.0669, decode.d2.loss_mask: 1.4477, decode.d2.loss_dice: 3.0942, decode.d3.loss_cls: 0.0627, decode.d3.loss_mask: 1.4460, decode.d3.loss_dice: 3.0956, decode.d4.loss_cls: 0.0579, decode.d4.loss_mask: 1.4415, decode.d4.loss_dice: 3.0859, decode.d5.loss_cls: 0.0638, decode.d5.loss_mask: 1.4462, decode.d5.loss_dice: 3.1071, decode.d6.loss_cls: 0.0552, decode.d6.loss_mask: 1.4461, decode.d6.loss_dice: 3.1007, decode.d7.loss_cls: 0.0620, decode.d7.loss_mask: 1.4473, decode.d7.loss_dice: 3.0950, decode.d8.loss_cls: 0.0606, decode.d8.loss_mask: 1.4456, decode.d8.loss_dice: 3.0923, loss: 50.1600
2023-03-17 20:35:21,235 - mmseg - INFO - Iter [2300/4000]	lr: 6.549e-06, eta: 0:16:07, time: 0.560, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0721, decode.loss_mask: 1.4364, decode.loss_dice: 3.0831, decode.d0.loss_cls: 4.1651, decode.d0.loss_mask: 1.4391, decode.d0.loss_dice: 3.0656, decode.d1.loss_cls: 0.1155, decode.d1.loss_mask: 1.4397, decode.d1.loss_dice: 3.0706, decode.d2.loss_cls: 0.0816, decode.d2.loss_mask: 1.4378, decode.d2.loss_dice: 3.0499, decode.d3.loss_cls: 0.0736, decode.d3.loss_mask: 1.4429, decode.d3.loss_dice: 3.0652, decode.d4.loss_cls: 0.0813, decode.d4.loss_mask: 1.4432, decode.d4.loss_dice: 3.0634, decode.d5.loss_cls: 0.0709, decode.d5.loss_mask: 1.4414, decode.d5.loss_dice: 3.0816, decode.d6.loss_cls: 0.0795, decode.d6.loss_mask: 1.4420, decode.d6.loss_dice: 3.0738, decode.d7.loss_cls: 0.0859, decode.d7.loss_mask: 1.4403, decode.d7.loss_dice: 3.0615, decode.d8.loss_cls: 0.0746, decode.d8.loss_mask: 1.4375, decode.d8.loss_dice: 3.0849, loss: 50.0000
2023-03-17 20:35:48,390 - mmseg - INFO - Iter [2350/4000]	lr: 6.356e-06, eta: 0:15:37, time: 0.543, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0595, decode.loss_mask: 1.4404, decode.loss_dice: 3.0893, decode.d0.loss_cls: 4.1198, decode.d0.loss_mask: 1.4238, decode.d0.loss_dice: 3.0669, decode.d1.loss_cls: 0.0887, decode.d1.loss_mask: 1.4497, decode.d1.loss_dice: 3.1262, decode.d2.loss_cls: 0.0733, decode.d2.loss_mask: 1.4412, decode.d2.loss_dice: 3.0861, decode.d3.loss_cls: 0.0547, decode.d3.loss_mask: 1.4456, decode.d3.loss_dice: 3.1148, decode.d4.loss_cls: 0.0527, decode.d4.loss_mask: 1.4466, decode.d4.loss_dice: 3.0859, decode.d5.loss_cls: 0.0450, decode.d5.loss_mask: 1.4443, decode.d5.loss_dice: 3.0818, decode.d6.loss_cls: 0.0539, decode.d6.loss_mask: 1.4455, decode.d6.loss_dice: 3.0768, decode.d7.loss_cls: 0.0498, decode.d7.loss_mask: 1.4431, decode.d7.loss_dice: 3.0833, decode.d8.loss_cls: 0.0415, decode.d8.loss_mask: 1.4450, decode.d8.loss_dice: 3.1075, loss: 49.9825
2023-03-17 20:36:15,743 - mmseg - INFO - Iter [2400/4000]	lr: 6.164e-06, eta: 0:15:08, time: 0.547, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0530, decode.loss_mask: 1.4782, decode.loss_dice: 3.1013, decode.d0.loss_cls: 4.0849, decode.d0.loss_mask: 1.4718, decode.d0.loss_dice: 3.0610, decode.d1.loss_cls: 0.0823, decode.d1.loss_mask: 1.4709, decode.d1.loss_dice: 3.0925, decode.d2.loss_cls: 0.0637, decode.d2.loss_mask: 1.4720, decode.d2.loss_dice: 3.0785, decode.d3.loss_cls: 0.0635, decode.d3.loss_mask: 1.4769, decode.d3.loss_dice: 3.0813, decode.d4.loss_cls: 0.0596, decode.d4.loss_mask: 1.4753, decode.d4.loss_dice: 3.0884, decode.d5.loss_cls: 0.0539, decode.d5.loss_mask: 1.4782, decode.d5.loss_dice: 3.0922, decode.d6.loss_cls: 0.0533, decode.d6.loss_mask: 1.4779, decode.d6.loss_dice: 3.0881, decode.d7.loss_cls: 0.0524, decode.d7.loss_mask: 1.4743, decode.d7.loss_dice: 3.0786, decode.d8.loss_cls: 0.0517, decode.d8.loss_mask: 1.4758, decode.d8.loss_dice: 3.1045, loss: 50.2358
2023-03-17 20:36:43,270 - mmseg - INFO - Iter [2450/4000]	lr: 5.971e-06, eta: 0:14:39, time: 0.551, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0432, decode.loss_mask: 1.4985, decode.loss_dice: 3.0814, decode.d0.loss_cls: 4.0341, decode.d0.loss_mask: 1.4896, decode.d0.loss_dice: 3.0546, decode.d1.loss_cls: 0.0685, decode.d1.loss_mask: 1.5027, decode.d1.loss_dice: 3.0904, decode.d2.loss_cls: 0.0521, decode.d2.loss_mask: 1.4983, decode.d2.loss_dice: 3.0732, decode.d3.loss_cls: 0.0452, decode.d3.loss_mask: 1.4988, decode.d3.loss_dice: 3.0960, decode.d4.loss_cls: 0.0431, decode.d4.loss_mask: 1.5000, decode.d4.loss_dice: 3.0819, decode.d5.loss_cls: 0.0419, decode.d5.loss_mask: 1.5011, decode.d5.loss_dice: 3.0739, decode.d6.loss_cls: 0.0426, decode.d6.loss_mask: 1.4988, decode.d6.loss_dice: 3.0957, decode.d7.loss_cls: 0.0427, decode.d7.loss_mask: 1.4981, decode.d7.loss_dice: 3.1111, decode.d8.loss_cls: 0.0433, decode.d8.loss_mask: 1.4998, decode.d8.loss_dice: 3.1037, loss: 50.3043
2023-03-17 20:37:11,886 - mmseg - INFO - Iter [2500/4000]	lr: 5.779e-06, eta: 0:14:11, time: 0.572, data_time: 0.049, memory: 14565, decode.loss_cls: 0.0456, decode.loss_mask: 1.4001, decode.loss_dice: 3.0909, decode.d0.loss_cls: 3.9959, decode.d0.loss_mask: 1.4024, decode.d0.loss_dice: 3.0435, decode.d1.loss_cls: 0.0775, decode.d1.loss_mask: 1.4010, decode.d1.loss_dice: 3.0940, decode.d2.loss_cls: 0.0600, decode.d2.loss_mask: 1.3963, decode.d2.loss_dice: 3.0857, decode.d3.loss_cls: 0.0541, decode.d3.loss_mask: 1.3999, decode.d3.loss_dice: 3.0723, decode.d4.loss_cls: 0.0554, decode.d4.loss_mask: 1.3978, decode.d4.loss_dice: 3.0637, decode.d5.loss_cls: 0.0491, decode.d5.loss_mask: 1.3983, decode.d5.loss_dice: 3.1064, decode.d6.loss_cls: 0.0495, decode.d6.loss_mask: 1.3982, decode.d6.loss_dice: 3.0805, decode.d7.loss_cls: 0.0547, decode.d7.loss_mask: 1.3987, decode.d7.loss_dice: 3.0813, decode.d8.loss_cls: 0.0494, decode.d8.loss_mask: 1.3983, decode.d8.loss_dice: 3.0759, loss: 49.2766
2023-03-17 20:37:39,064 - mmseg - INFO - Iter [2550/4000]	lr: 5.586e-06, eta: 0:13:42, time: 0.544, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0468, decode.loss_mask: 1.4501, decode.loss_dice: 3.0916, decode.d0.loss_cls: 3.9469, decode.d0.loss_mask: 1.4462, decode.d0.loss_dice: 3.0861, decode.d1.loss_cls: 0.0805, decode.d1.loss_mask: 1.4508, decode.d1.loss_dice: 3.1133, decode.d2.loss_cls: 0.0543, decode.d2.loss_mask: 1.4497, decode.d2.loss_dice: 3.0949, decode.d3.loss_cls: 0.0430, decode.d3.loss_mask: 1.4541, decode.d3.loss_dice: 3.1203, decode.d4.loss_cls: 0.0515, decode.d4.loss_mask: 1.4460, decode.d4.loss_dice: 3.0812, decode.d5.loss_cls: 0.0483, decode.d5.loss_mask: 1.4444, decode.d5.loss_dice: 3.1133, decode.d6.loss_cls: 0.0397, decode.d6.loss_mask: 1.4476, decode.d6.loss_dice: 3.1027, decode.d7.loss_cls: 0.0468, decode.d7.loss_mask: 1.4440, decode.d7.loss_dice: 3.1165, decode.d8.loss_cls: 0.0393, decode.d8.loss_mask: 1.4502, decode.d8.loss_dice: 3.1140, loss: 49.9142
2023-03-17 20:38:06,442 - mmseg - INFO - Iter [2600/4000]	lr: 5.394e-06, eta: 0:13:13, time: 0.548, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0489, decode.loss_mask: 1.4157, decode.loss_dice: 3.0575, decode.d0.loss_cls: 3.9458, decode.d0.loss_mask: 1.4022, decode.d0.loss_dice: 3.0299, decode.d1.loss_cls: 0.0850, decode.d1.loss_mask: 1.4107, decode.d1.loss_dice: 3.1043, decode.d2.loss_cls: 0.0604, decode.d2.loss_mask: 1.4104, decode.d2.loss_dice: 3.0654, decode.d3.loss_cls: 0.0550, decode.d3.loss_mask: 1.4126, decode.d3.loss_dice: 3.0642, decode.d4.loss_cls: 0.0452, decode.d4.loss_mask: 1.4142, decode.d4.loss_dice: 3.0693, decode.d5.loss_cls: 0.0497, decode.d5.loss_mask: 1.4120, decode.d5.loss_dice: 3.0591, decode.d6.loss_cls: 0.0480, decode.d6.loss_mask: 1.4140, decode.d6.loss_dice: 3.0739, decode.d7.loss_cls: 0.0472, decode.d7.loss_mask: 1.4171, decode.d7.loss_dice: 3.0603, decode.d8.loss_cls: 0.0470, decode.d8.loss_mask: 1.4165, decode.d8.loss_dice: 3.0753, loss: 49.2166
2023-03-17 20:38:33,972 - mmseg - INFO - Iter [2650/4000]	lr: 5.201e-06, eta: 0:12:44, time: 0.551, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0539, decode.loss_mask: 1.3755, decode.loss_dice: 3.0710, decode.d0.loss_cls: 3.8841, decode.d0.loss_mask: 1.3884, decode.d0.loss_dice: 3.0324, decode.d1.loss_cls: 0.0832, decode.d1.loss_mask: 1.3798, decode.d1.loss_dice: 3.0694, decode.d2.loss_cls: 0.0706, decode.d2.loss_mask: 1.3726, decode.d2.loss_dice: 3.0639, decode.d3.loss_cls: 0.0643, decode.d3.loss_mask: 1.3779, decode.d3.loss_dice: 3.0425, decode.d4.loss_cls: 0.0537, decode.d4.loss_mask: 1.3765, decode.d4.loss_dice: 3.0713, decode.d5.loss_cls: 0.0547, decode.d5.loss_mask: 1.3788, decode.d5.loss_dice: 3.0466, decode.d6.loss_cls: 0.0545, decode.d6.loss_mask: 1.3771, decode.d6.loss_dice: 3.0750, decode.d7.loss_cls: 0.0461, decode.d7.loss_mask: 1.3794, decode.d7.loss_dice: 3.0589, decode.d8.loss_cls: 0.0507, decode.d8.loss_mask: 1.3762, decode.d8.loss_dice: 3.0734, loss: 48.8027
2023-03-17 20:39:00,975 - mmseg - INFO - Iter [2700/4000]	lr: 5.009e-06, eta: 0:12:15, time: 0.540, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0345, decode.loss_mask: 1.4932, decode.loss_dice: 3.0395, decode.d0.loss_cls: 3.8664, decode.d0.loss_mask: 1.4939, decode.d0.loss_dice: 3.0134, decode.d1.loss_cls: 0.0610, decode.d1.loss_mask: 1.4988, decode.d1.loss_dice: 3.0349, decode.d2.loss_cls: 0.0437, decode.d2.loss_mask: 1.4964, decode.d2.loss_dice: 3.0307, decode.d3.loss_cls: 0.0375, decode.d3.loss_mask: 1.4940, decode.d3.loss_dice: 3.0167, decode.d4.loss_cls: 0.0361, decode.d4.loss_mask: 1.4940, decode.d4.loss_dice: 3.0244, decode.d5.loss_cls: 0.0352, decode.d5.loss_mask: 1.4924, decode.d5.loss_dice: 3.0500, decode.d6.loss_cls: 0.0350, decode.d6.loss_mask: 1.4921, decode.d6.loss_dice: 3.0470, decode.d7.loss_cls: 0.0348, decode.d7.loss_mask: 1.4938, decode.d7.loss_dice: 3.0210, decode.d8.loss_cls: 0.0342, decode.d8.loss_mask: 1.4938, decode.d8.loss_dice: 3.0410, loss: 49.4795
2023-03-17 20:39:28,511 - mmseg - INFO - Iter [2750/4000]	lr: 4.816e-06, eta: 0:11:47, time: 0.551, data_time: 0.007, memory: 14565, decode.loss_cls: 0.0766, decode.loss_mask: 1.4482, decode.loss_dice: 3.1211, decode.d0.loss_cls: 3.7998, decode.d0.loss_mask: 1.4420, decode.d0.loss_dice: 3.0863, decode.d1.loss_cls: 0.0892, decode.d1.loss_mask: 1.4520, decode.d1.loss_dice: 3.1137, decode.d2.loss_cls: 0.0653, decode.d2.loss_mask: 1.4539, decode.d2.loss_dice: 3.1155, decode.d3.loss_cls: 0.0716, decode.d3.loss_mask: 1.4455, decode.d3.loss_dice: 3.1023, decode.d4.loss_cls: 0.0775, decode.d4.loss_mask: 1.4447, decode.d4.loss_dice: 3.1081, decode.d5.loss_cls: 0.0738, decode.d5.loss_mask: 1.4450, decode.d5.loss_dice: 3.1056, decode.d6.loss_cls: 0.0631, decode.d6.loss_mask: 1.4474, decode.d6.loss_dice: 3.1098, decode.d7.loss_cls: 0.0648, decode.d7.loss_mask: 1.4504, decode.d7.loss_dice: 3.0993, decode.d8.loss_cls: 0.0623, decode.d8.loss_mask: 1.4498, decode.d8.loss_dice: 3.1110, loss: 49.9959
2023-03-17 20:39:56,858 - mmseg - INFO - Iter [2800/4000]	lr: 4.624e-06, eta: 0:11:19, time: 0.567, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0406, decode.loss_mask: 1.3610, decode.loss_dice: 3.1016, decode.d0.loss_cls: 3.8074, decode.d0.loss_mask: 1.3632, decode.d0.loss_dice: 3.0772, decode.d1.loss_cls: 0.0701, decode.d1.loss_mask: 1.3621, decode.d1.loss_dice: 3.0823, decode.d2.loss_cls: 0.0525, decode.d2.loss_mask: 1.3622, decode.d2.loss_dice: 3.0777, decode.d3.loss_cls: 0.0494, decode.d3.loss_mask: 1.3616, decode.d3.loss_dice: 3.0652, decode.d4.loss_cls: 0.0492, decode.d4.loss_mask: 1.3615, decode.d4.loss_dice: 3.0916, decode.d5.loss_cls: 0.0385, decode.d5.loss_mask: 1.3627, decode.d5.loss_dice: 3.0782, decode.d6.loss_cls: 0.0408, decode.d6.loss_mask: 1.3597, decode.d6.loss_dice: 3.0849, decode.d7.loss_cls: 0.0398, decode.d7.loss_mask: 1.3630, decode.d7.loss_dice: 3.0500, decode.d8.loss_cls: 0.0388, decode.d8.loss_mask: 1.3600, decode.d8.loss_dice: 3.0776, loss: 48.6306
2023-03-17 20:40:24,759 - mmseg - INFO - Iter [2850/4000]	lr: 4.431e-06, eta: 0:10:50, time: 0.558, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0932, decode.loss_mask: 1.3966, decode.loss_dice: 3.0847, decode.d0.loss_cls: 3.7720, decode.d0.loss_mask: 1.3988, decode.d0.loss_dice: 3.0240, decode.d1.loss_cls: 0.1231, decode.d1.loss_mask: 1.3958, decode.d1.loss_dice: 3.0816, decode.d2.loss_cls: 0.0939, decode.d2.loss_mask: 1.3957, decode.d2.loss_dice: 3.0781, decode.d3.loss_cls: 0.0969, decode.d3.loss_mask: 1.3935, decode.d3.loss_dice: 3.0588, decode.d4.loss_cls: 0.0911, decode.d4.loss_mask: 1.3932, decode.d4.loss_dice: 3.0574, decode.d5.loss_cls: 0.0868, decode.d5.loss_mask: 1.3927, decode.d5.loss_dice: 3.0654, decode.d6.loss_cls: 0.1032, decode.d6.loss_mask: 1.3920, decode.d6.loss_dice: 3.0462, decode.d7.loss_cls: 0.0965, decode.d7.loss_mask: 1.3939, decode.d7.loss_dice: 3.0890, decode.d8.loss_cls: 0.1050, decode.d8.loss_mask: 1.3908, decode.d8.loss_dice: 3.0703, loss: 49.2600
2023-03-17 20:40:52,347 - mmseg - INFO - Iter [2900/4000]	lr: 4.239e-06, eta: 0:10:22, time: 0.552, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0638, decode.loss_mask: 1.3295, decode.loss_dice: 3.0619, decode.d0.loss_cls: 3.6995, decode.d0.loss_mask: 1.3251, decode.d0.loss_dice: 3.0371, decode.d1.loss_cls: 0.0745, decode.d1.loss_mask: 1.3269, decode.d1.loss_dice: 3.0806, decode.d2.loss_cls: 0.0564, decode.d2.loss_mask: 1.3290, decode.d2.loss_dice: 3.0765, decode.d3.loss_cls: 0.0578, decode.d3.loss_mask: 1.3277, decode.d3.loss_dice: 3.0735, decode.d4.loss_cls: 0.0530, decode.d4.loss_mask: 1.3261, decode.d4.loss_dice: 3.0526, decode.d5.loss_cls: 0.0479, decode.d5.loss_mask: 1.3294, decode.d5.loss_dice: 3.0735, decode.d6.loss_cls: 0.0603, decode.d6.loss_mask: 1.3289, decode.d6.loss_dice: 3.0817, decode.d7.loss_cls: 0.0493, decode.d7.loss_mask: 1.3319, decode.d7.loss_dice: 3.0806, decode.d8.loss_cls: 0.0606, decode.d8.loss_mask: 1.3300, decode.d8.loss_dice: 3.0896, loss: 48.2151
2023-03-17 20:41:20,842 - mmseg - INFO - Iter [2950/4000]	lr: 4.046e-06, eta: 0:09:53, time: 0.570, data_time: 0.049, memory: 14565, decode.loss_cls: 0.0549, decode.loss_mask: 1.4048, decode.loss_dice: 3.0577, decode.d0.loss_cls: 3.6554, decode.d0.loss_mask: 1.4035, decode.d0.loss_dice: 3.0346, decode.d1.loss_cls: 0.0664, decode.d1.loss_mask: 1.4054, decode.d1.loss_dice: 3.0887, decode.d2.loss_cls: 0.0698, decode.d2.loss_mask: 1.4048, decode.d2.loss_dice: 3.0643, decode.d3.loss_cls: 0.0584, decode.d3.loss_mask: 1.4034, decode.d3.loss_dice: 3.0616, decode.d4.loss_cls: 0.0543, decode.d4.loss_mask: 1.4052, decode.d4.loss_dice: 3.0762, decode.d5.loss_cls: 0.0513, decode.d5.loss_mask: 1.4047, decode.d5.loss_dice: 3.0631, decode.d6.loss_cls: 0.0554, decode.d6.loss_mask: 1.4065, decode.d6.loss_dice: 3.0477, decode.d7.loss_cls: 0.0563, decode.d7.loss_mask: 1.4041, decode.d7.loss_dice: 3.0769, decode.d8.loss_cls: 0.0503, decode.d8.loss_mask: 1.4050, decode.d8.loss_dice: 3.0736, loss: 48.8642
2023-03-17 20:41:48,147 - mmseg - INFO - Saving checkpoint at 3000 iterations
2023-03-17 20:41:53,392 - mmseg - INFO - Exp name: mask2former_beit_adapter_base_480_40k_pascal_context_59_ss.py
2023-03-17 20:41:53,398 - mmseg - INFO - Iter [3000/4000]	lr: 3.854e-06, eta: 0:09:26, time: 0.651, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0369, decode.loss_mask: 1.4707, decode.loss_dice: 3.0956, decode.d0.loss_cls: 3.7055, decode.d0.loss_mask: 1.4568, decode.d0.loss_dice: 3.0486, decode.d1.loss_cls: 0.0656, decode.d1.loss_mask: 1.4694, decode.d1.loss_dice: 3.0669, decode.d2.loss_cls: 0.0523, decode.d2.loss_mask: 1.4667, decode.d2.loss_dice: 3.0843, decode.d3.loss_cls: 0.0395, decode.d3.loss_mask: 1.4721, decode.d3.loss_dice: 3.0655, decode.d4.loss_cls: 0.0386, decode.d4.loss_mask: 1.4669, decode.d4.loss_dice: 3.0850, decode.d5.loss_cls: 0.0405, decode.d5.loss_mask: 1.4699, decode.d5.loss_dice: 3.0793, decode.d6.loss_cls: 0.0366, decode.d6.loss_mask: 1.4707, decode.d6.loss_dice: 3.0908, decode.d7.loss_cls: 0.0365, decode.d7.loss_mask: 1.4697, decode.d7.loss_dice: 3.0880, decode.d8.loss_cls: 0.0404, decode.d8.loss_mask: 1.4715, decode.d8.loss_dice: 3.0859, loss: 49.5668
2023-03-17 20:42:20,768 - mmseg - INFO - Iter [3050/4000]	lr: 3.661e-06, eta: 0:08:58, time: 0.548, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0813, decode.loss_mask: 1.4080, decode.loss_dice: 3.0949, decode.d0.loss_cls: 3.6532, decode.d0.loss_mask: 1.3963, decode.d0.loss_dice: 3.0638, decode.d1.loss_cls: 0.0981, decode.d1.loss_mask: 1.4045, decode.d1.loss_dice: 3.0866, decode.d2.loss_cls: 0.0825, decode.d2.loss_mask: 1.4031, decode.d2.loss_dice: 3.0734, decode.d3.loss_cls: 0.0881, decode.d3.loss_mask: 1.4017, decode.d3.loss_dice: 3.0819, decode.d4.loss_cls: 0.0881, decode.d4.loss_mask: 1.4071, decode.d4.loss_dice: 3.0744, decode.d5.loss_cls: 0.0928, decode.d5.loss_mask: 1.4055, decode.d5.loss_dice: 3.0732, decode.d6.loss_cls: 0.0824, decode.d6.loss_mask: 1.4024, decode.d6.loss_dice: 3.0651, decode.d7.loss_cls: 0.0856, decode.d7.loss_mask: 1.4065, decode.d7.loss_dice: 3.0754, decode.d8.loss_cls: 0.0730, decode.d8.loss_mask: 1.4070, decode.d8.loss_dice: 3.0755, loss: 49.2315
2023-03-17 20:42:49,120 - mmseg - INFO - Iter [3100/4000]	lr: 3.469e-06, eta: 0:08:29, time: 0.567, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0864, decode.loss_mask: 1.4581, decode.loss_dice: 3.0751, decode.d0.loss_cls: 3.6487, decode.d0.loss_mask: 1.4431, decode.d0.loss_dice: 3.0317, decode.d1.loss_cls: 0.1026, decode.d1.loss_mask: 1.4569, decode.d1.loss_dice: 3.0715, decode.d2.loss_cls: 0.0973, decode.d2.loss_mask: 1.4615, decode.d2.loss_dice: 3.0722, decode.d3.loss_cls: 0.0966, decode.d3.loss_mask: 1.4620, decode.d3.loss_dice: 3.0562, decode.d4.loss_cls: 0.0890, decode.d4.loss_mask: 1.4610, decode.d4.loss_dice: 3.0746, decode.d5.loss_cls: 0.0888, decode.d5.loss_mask: 1.4608, decode.d5.loss_dice: 3.0581, decode.d6.loss_cls: 0.0822, decode.d6.loss_mask: 1.4592, decode.d6.loss_dice: 3.0754, decode.d7.loss_cls: 0.0901, decode.d7.loss_mask: 1.4611, decode.d7.loss_dice: 3.0637, decode.d8.loss_cls: 0.0737, decode.d8.loss_mask: 1.4577, decode.d8.loss_dice: 3.0677, loss: 49.6831
2023-03-17 20:43:15,870 - mmseg - INFO - Iter [3150/4000]	lr: 3.276e-06, eta: 0:08:01, time: 0.535, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0535, decode.loss_mask: 1.5050, decode.loss_dice: 3.0857, decode.d0.loss_cls: 3.5550, decode.d0.loss_mask: 1.4840, decode.d0.loss_dice: 3.0947, decode.d1.loss_cls: 0.0810, decode.d1.loss_mask: 1.5056, decode.d1.loss_dice: 3.0957, decode.d2.loss_cls: 0.0880, decode.d2.loss_mask: 1.5007, decode.d2.loss_dice: 3.0941, decode.d3.loss_cls: 0.0656, decode.d3.loss_mask: 1.5019, decode.d3.loss_dice: 3.0932, decode.d4.loss_cls: 0.0485, decode.d4.loss_mask: 1.5021, decode.d4.loss_dice: 3.1085, decode.d5.loss_cls: 0.0472, decode.d5.loss_mask: 1.5027, decode.d5.loss_dice: 3.0966, decode.d6.loss_cls: 0.0453, decode.d6.loss_mask: 1.5027, decode.d6.loss_dice: 3.1062, decode.d7.loss_cls: 0.0478, decode.d7.loss_mask: 1.5043, decode.d7.loss_dice: 3.1230, decode.d8.loss_cls: 0.0543, decode.d8.loss_mask: 1.5017, decode.d8.loss_dice: 3.0919, loss: 50.0867
2023-03-17 20:43:42,601 - mmseg - INFO - Iter [3200/4000]	lr: 3.084e-06, eta: 0:07:32, time: 0.535, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0529, decode.loss_mask: 1.3839, decode.loss_dice: 3.0788, decode.d0.loss_cls: 3.5629, decode.d0.loss_mask: 1.3957, decode.d0.loss_dice: 3.0492, decode.d1.loss_cls: 0.0636, decode.d1.loss_mask: 1.3933, decode.d1.loss_dice: 3.0688, decode.d2.loss_cls: 0.0542, decode.d2.loss_mask: 1.3884, decode.d2.loss_dice: 3.0738, decode.d3.loss_cls: 0.0470, decode.d3.loss_mask: 1.3875, decode.d3.loss_dice: 3.0481, decode.d4.loss_cls: 0.0538, decode.d4.loss_mask: 1.3896, decode.d4.loss_dice: 3.0694, decode.d5.loss_cls: 0.0486, decode.d5.loss_mask: 1.3873, decode.d5.loss_dice: 3.0611, decode.d6.loss_cls: 0.0500, decode.d6.loss_mask: 1.3856, decode.d6.loss_dice: 3.0420, decode.d7.loss_cls: 0.0568, decode.d7.loss_mask: 1.3855, decode.d7.loss_dice: 3.0502, decode.d8.loss_cls: 0.0531, decode.d8.loss_mask: 1.3857, decode.d8.loss_dice: 3.0511, loss: 48.5178
2023-03-17 20:44:10,005 - mmseg - INFO - Iter [3250/4000]	lr: 2.891e-06, eta: 0:07:04, time: 0.548, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0460, decode.loss_mask: 1.4502, decode.loss_dice: 3.1440, decode.d0.loss_cls: 3.5740, decode.d0.loss_mask: 1.4394, decode.d0.loss_dice: 3.0875, decode.d1.loss_cls: 0.0665, decode.d1.loss_mask: 1.4518, decode.d1.loss_dice: 3.1193, decode.d2.loss_cls: 0.0544, decode.d2.loss_mask: 1.4520, decode.d2.loss_dice: 3.1110, decode.d3.loss_cls: 0.0454, decode.d3.loss_mask: 1.4522, decode.d3.loss_dice: 3.1180, decode.d4.loss_cls: 0.0495, decode.d4.loss_mask: 1.4480, decode.d4.loss_dice: 3.1224, decode.d5.loss_cls: 0.0486, decode.d5.loss_mask: 1.4517, decode.d5.loss_dice: 3.1167, decode.d6.loss_cls: 0.0472, decode.d6.loss_mask: 1.4480, decode.d6.loss_dice: 3.1379, decode.d7.loss_cls: 0.0463, decode.d7.loss_mask: 1.4467, decode.d7.loss_dice: 3.0972, decode.d8.loss_cls: 0.0538, decode.d8.loss_mask: 1.4482, decode.d8.loss_dice: 3.1276, loss: 49.7015
2023-03-17 20:44:37,675 - mmseg - INFO - Iter [3300/4000]	lr: 2.699e-06, eta: 0:06:35, time: 0.553, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0418, decode.loss_mask: 1.4665, decode.loss_dice: 3.0585, decode.d0.loss_cls: 3.5079, decode.d0.loss_mask: 1.4638, decode.d0.loss_dice: 3.0540, decode.d1.loss_cls: 0.0697, decode.d1.loss_mask: 1.4668, decode.d1.loss_dice: 3.0601, decode.d2.loss_cls: 0.0515, decode.d2.loss_mask: 1.4638, decode.d2.loss_dice: 3.0573, decode.d3.loss_cls: 0.0450, decode.d3.loss_mask: 1.4640, decode.d3.loss_dice: 3.0263, decode.d4.loss_cls: 0.0375, decode.d4.loss_mask: 1.4635, decode.d4.loss_dice: 3.0879, decode.d5.loss_cls: 0.0385, decode.d5.loss_mask: 1.4647, decode.d5.loss_dice: 3.0576, decode.d6.loss_cls: 0.0421, decode.d6.loss_mask: 1.4669, decode.d6.loss_dice: 3.0572, decode.d7.loss_cls: 0.0423, decode.d7.loss_mask: 1.4659, decode.d7.loss_dice: 3.0591, decode.d8.loss_cls: 0.0393, decode.d8.loss_mask: 1.4647, decode.d8.loss_dice: 3.0848, loss: 49.1692
2023-03-17 20:45:06,532 - mmseg - INFO - Iter [3350/4000]	lr: 2.506e-06, eta: 0:06:07, time: 0.577, data_time: 0.049, memory: 14565, decode.loss_cls: 0.0504, decode.loss_mask: 1.3854, decode.loss_dice: 3.0427, decode.d0.loss_cls: 3.4774, decode.d0.loss_mask: 1.3801, decode.d0.loss_dice: 3.0302, decode.d1.loss_cls: 0.0598, decode.d1.loss_mask: 1.3880, decode.d1.loss_dice: 3.0840, decode.d2.loss_cls: 0.0570, decode.d2.loss_mask: 1.3804, decode.d2.loss_dice: 3.0457, decode.d3.loss_cls: 0.0488, decode.d3.loss_mask: 1.3863, decode.d3.loss_dice: 3.0656, decode.d4.loss_cls: 0.0469, decode.d4.loss_mask: 1.3856, decode.d4.loss_dice: 3.0624, decode.d5.loss_cls: 0.0469, decode.d5.loss_mask: 1.3866, decode.d5.loss_dice: 3.0568, decode.d6.loss_cls: 0.0482, decode.d6.loss_mask: 1.3855, decode.d6.loss_dice: 3.0661, decode.d7.loss_cls: 0.0492, decode.d7.loss_mask: 1.3846, decode.d7.loss_dice: 3.0727, decode.d8.loss_cls: 0.0485, decode.d8.loss_mask: 1.3820, decode.d8.loss_dice: 3.0692, loss: 48.3729
2023-03-17 20:45:33,729 - mmseg - INFO - Iter [3400/4000]	lr: 2.314e-06, eta: 0:05:39, time: 0.544, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0537, decode.loss_mask: 1.3699, decode.loss_dice: 3.0713, decode.d0.loss_cls: 3.4438, decode.d0.loss_mask: 1.3525, decode.d0.loss_dice: 3.0338, decode.d1.loss_cls: 0.0781, decode.d1.loss_mask: 1.3696, decode.d1.loss_dice: 3.0787, decode.d2.loss_cls: 0.0594, decode.d2.loss_mask: 1.3681, decode.d2.loss_dice: 3.0759, decode.d3.loss_cls: 0.0583, decode.d3.loss_mask: 1.3652, decode.d3.loss_dice: 3.0828, decode.d4.loss_cls: 0.0547, decode.d4.loss_mask: 1.3659, decode.d4.loss_dice: 3.0855, decode.d5.loss_cls: 0.0514, decode.d5.loss_mask: 1.3661, decode.d5.loss_dice: 3.0628, decode.d6.loss_cls: 0.0479, decode.d6.loss_mask: 1.3668, decode.d6.loss_dice: 3.0640, decode.d7.loss_cls: 0.0485, decode.d7.loss_mask: 1.3681, decode.d7.loss_dice: 3.0470, decode.d8.loss_cls: 0.0539, decode.d8.loss_mask: 1.3689, decode.d8.loss_dice: 3.0699, loss: 48.2826
2023-03-17 20:46:00,841 - mmseg - INFO - Iter [3450/4000]	lr: 2.121e-06, eta: 0:05:10, time: 0.542, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0520, decode.loss_mask: 1.4789, decode.loss_dice: 3.0925, decode.d0.loss_cls: 3.4252, decode.d0.loss_mask: 1.4641, decode.d0.loss_dice: 3.0688, decode.d1.loss_cls: 0.0608, decode.d1.loss_mask: 1.4842, decode.d1.loss_dice: 3.0700, decode.d2.loss_cls: 0.0506, decode.d2.loss_mask: 1.4788, decode.d2.loss_dice: 3.0915, decode.d3.loss_cls: 0.0516, decode.d3.loss_mask: 1.4818, decode.d3.loss_dice: 3.0759, decode.d4.loss_cls: 0.0462, decode.d4.loss_mask: 1.4780, decode.d4.loss_dice: 3.0665, decode.d5.loss_cls: 0.0466, decode.d5.loss_mask: 1.4778, decode.d5.loss_dice: 3.0696, decode.d6.loss_cls: 0.0469, decode.d6.loss_mask: 1.4811, decode.d6.loss_dice: 3.0727, decode.d7.loss_cls: 0.0504, decode.d7.loss_mask: 1.4791, decode.d7.loss_dice: 3.0761, decode.d8.loss_cls: 0.0476, decode.d8.loss_mask: 1.4782, decode.d8.loss_dice: 3.0776, loss: 49.4212
2023-03-17 20:46:28,089 - mmseg - INFO - Iter [3500/4000]	lr: 1.929e-06, eta: 0:04:42, time: 0.545, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0404, decode.loss_mask: 1.4327, decode.loss_dice: 3.0785, decode.d0.loss_cls: 3.4476, decode.d0.loss_mask: 1.4225, decode.d0.loss_dice: 3.0879, decode.d1.loss_cls: 0.0636, decode.d1.loss_mask: 1.4320, decode.d1.loss_dice: 3.0776, decode.d2.loss_cls: 0.0508, decode.d2.loss_mask: 1.4308, decode.d2.loss_dice: 3.0948, decode.d3.loss_cls: 0.0418, decode.d3.loss_mask: 1.4310, decode.d3.loss_dice: 3.0728, decode.d4.loss_cls: 0.0429, decode.d4.loss_mask: 1.4311, decode.d4.loss_dice: 3.0623, decode.d5.loss_cls: 0.0419, decode.d5.loss_mask: 1.4307, decode.d5.loss_dice: 3.0841, decode.d6.loss_cls: 0.0414, decode.d6.loss_mask: 1.4325, decode.d6.loss_dice: 3.0885, decode.d7.loss_cls: 0.0411, decode.d7.loss_mask: 1.4306, decode.d7.loss_dice: 3.0831, decode.d8.loss_cls: 0.0391, decode.d8.loss_mask: 1.4330, decode.d8.loss_dice: 3.0597, loss: 48.9470
2023-03-17 20:46:55,363 - mmseg - INFO - Iter [3550/4000]	lr: 1.736e-06, eta: 0:04:13, time: 0.545, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0530, decode.loss_mask: 1.4203, decode.loss_dice: 3.0321, decode.d0.loss_cls: 3.4656, decode.d0.loss_mask: 1.4229, decode.d0.loss_dice: 2.9893, decode.d1.loss_cls: 0.0732, decode.d1.loss_mask: 1.4253, decode.d1.loss_dice: 3.0489, decode.d2.loss_cls: 0.0620, decode.d2.loss_mask: 1.4248, decode.d2.loss_dice: 3.0248, decode.d3.loss_cls: 0.0568, decode.d3.loss_mask: 1.4244, decode.d3.loss_dice: 3.0355, decode.d4.loss_cls: 0.0537, decode.d4.loss_mask: 1.4227, decode.d4.loss_dice: 3.0141, decode.d5.loss_cls: 0.0526, decode.d5.loss_mask: 1.4244, decode.d5.loss_dice: 3.0494, decode.d6.loss_cls: 0.0520, decode.d6.loss_mask: 1.4230, decode.d6.loss_dice: 3.0304, decode.d7.loss_cls: 0.0519, decode.d7.loss_mask: 1.4221, decode.d7.loss_dice: 3.0213, decode.d8.loss_cls: 0.0523, decode.d8.loss_mask: 1.4218, decode.d8.loss_dice: 3.0072, loss: 48.4578
2023-03-17 20:47:21,948 - mmseg - INFO - Iter [3600/4000]	lr: 1.544e-06, eta: 0:03:45, time: 0.532, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0714, decode.loss_mask: 1.4705, decode.loss_dice: 3.0644, decode.d0.loss_cls: 3.3687, decode.d0.loss_mask: 1.4607, decode.d0.loss_dice: 3.0556, decode.d1.loss_cls: 0.1064, decode.d1.loss_mask: 1.4677, decode.d1.loss_dice: 3.0734, decode.d2.loss_cls: 0.0732, decode.d2.loss_mask: 1.4725, decode.d2.loss_dice: 3.0905, decode.d3.loss_cls: 0.0666, decode.d3.loss_mask: 1.4744, decode.d3.loss_dice: 3.0905, decode.d4.loss_cls: 0.0661, decode.d4.loss_mask: 1.4730, decode.d4.loss_dice: 3.0755, decode.d5.loss_cls: 0.0536, decode.d5.loss_mask: 1.4745, decode.d5.loss_dice: 3.0639, decode.d6.loss_cls: 0.0749, decode.d6.loss_mask: 1.4734, decode.d6.loss_dice: 3.0630, decode.d7.loss_cls: 0.0679, decode.d7.loss_mask: 1.4720, decode.d7.loss_dice: 3.0463, decode.d8.loss_cls: 0.0782, decode.d8.loss_mask: 1.4682, decode.d8.loss_dice: 3.0986, loss: 49.4557
2023-03-17 20:47:49,505 - mmseg - INFO - Iter [3650/4000]	lr: 1.351e-06, eta: 0:03:17, time: 0.551, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0438, decode.loss_mask: 1.4660, decode.loss_dice: 3.0683, decode.d0.loss_cls: 3.3848, decode.d0.loss_mask: 1.4615, decode.d0.loss_dice: 3.0499, decode.d1.loss_cls: 0.0610, decode.d1.loss_mask: 1.4715, decode.d1.loss_dice: 3.0766, decode.d2.loss_cls: 0.0540, decode.d2.loss_mask: 1.4695, decode.d2.loss_dice: 3.0682, decode.d3.loss_cls: 0.0574, decode.d3.loss_mask: 1.4684, decode.d3.loss_dice: 3.0852, decode.d4.loss_cls: 0.0521, decode.d4.loss_mask: 1.4652, decode.d4.loss_dice: 3.0721, decode.d5.loss_cls: 0.0493, decode.d5.loss_mask: 1.4667, decode.d5.loss_dice: 3.0572, decode.d6.loss_cls: 0.0467, decode.d6.loss_mask: 1.4656, decode.d6.loss_dice: 3.0709, decode.d7.loss_cls: 0.0454, decode.d7.loss_mask: 1.4682, decode.d7.loss_dice: 3.0894, decode.d8.loss_cls: 0.0419, decode.d8.loss_mask: 1.4657, decode.d8.loss_dice: 3.1063, loss: 49.2488
2023-03-17 20:48:17,047 - mmseg - INFO - Iter [3700/4000]	lr: 1.159e-06, eta: 0:02:49, time: 0.551, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0522, decode.loss_mask: 1.3722, decode.loss_dice: 3.0855, decode.d0.loss_cls: 3.3407, decode.d0.loss_mask: 1.3837, decode.d0.loss_dice: 3.0604, decode.d1.loss_cls: 0.0755, decode.d1.loss_mask: 1.3746, decode.d1.loss_dice: 3.0723, decode.d2.loss_cls: 0.0671, decode.d2.loss_mask: 1.3746, decode.d2.loss_dice: 3.0747, decode.d3.loss_cls: 0.0622, decode.d3.loss_mask: 1.3743, decode.d3.loss_dice: 3.0602, decode.d4.loss_cls: 0.0522, decode.d4.loss_mask: 1.3736, decode.d4.loss_dice: 3.0618, decode.d5.loss_cls: 0.0519, decode.d5.loss_mask: 1.3719, decode.d5.loss_dice: 3.0595, decode.d6.loss_cls: 0.0489, decode.d6.loss_mask: 1.3734, decode.d6.loss_dice: 3.0606, decode.d7.loss_cls: 0.0512, decode.d7.loss_mask: 1.3734, decode.d7.loss_dice: 3.0836, decode.d8.loss_cls: 0.0521, decode.d8.loss_mask: 1.3720, decode.d8.loss_dice: 3.0788, loss: 48.2952
2023-03-17 20:48:47,033 - mmseg - INFO - Iter [3750/4000]	lr: 9.664e-07, eta: 0:02:20, time: 0.600, data_time: 0.049, memory: 14565, decode.loss_cls: 0.0408, decode.loss_mask: 1.4987, decode.loss_dice: 3.0799, decode.d0.loss_cls: 3.3559, decode.d0.loss_mask: 1.4994, decode.d0.loss_dice: 3.0671, decode.d1.loss_cls: 0.0639, decode.d1.loss_mask: 1.4988, decode.d1.loss_dice: 3.0774, decode.d2.loss_cls: 0.0513, decode.d2.loss_mask: 1.5004, decode.d2.loss_dice: 3.0883, decode.d3.loss_cls: 0.0508, decode.d3.loss_mask: 1.4990, decode.d3.loss_dice: 3.0661, decode.d4.loss_cls: 0.0492, decode.d4.loss_mask: 1.4995, decode.d4.loss_dice: 3.0847, decode.d5.loss_cls: 0.0469, decode.d5.loss_mask: 1.4991, decode.d5.loss_dice: 3.0799, decode.d6.loss_cls: 0.0454, decode.d6.loss_mask: 1.5002, decode.d6.loss_dice: 3.0762, decode.d7.loss_cls: 0.0445, decode.d7.loss_mask: 1.4976, decode.d7.loss_dice: 3.0820, decode.d8.loss_cls: 0.0403, decode.d8.loss_mask: 1.4982, decode.d8.loss_dice: 3.0736, loss: 49.5551
2023-03-17 20:49:13,552 - mmseg - INFO - Iter [3800/4000]	lr: 7.739e-07, eta: 0:01:52, time: 0.530, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0485, decode.loss_mask: 1.3980, decode.loss_dice: 3.0735, decode.d0.loss_cls: 3.3506, decode.d0.loss_mask: 1.3920, decode.d0.loss_dice: 3.0605, decode.d1.loss_cls: 0.0654, decode.d1.loss_mask: 1.4016, decode.d1.loss_dice: 3.0727, decode.d2.loss_cls: 0.0633, decode.d2.loss_mask: 1.3982, decode.d2.loss_dice: 3.0697, decode.d3.loss_cls: 0.0596, decode.d3.loss_mask: 1.3971, decode.d3.loss_dice: 3.0760, decode.d4.loss_cls: 0.0496, decode.d4.loss_mask: 1.3971, decode.d4.loss_dice: 3.0737, decode.d5.loss_cls: 0.0474, decode.d5.loss_mask: 1.4000, decode.d5.loss_dice: 3.0997, decode.d6.loss_cls: 0.0502, decode.d6.loss_mask: 1.3978, decode.d6.loss_dice: 3.0921, decode.d7.loss_cls: 0.0556, decode.d7.loss_mask: 1.3998, decode.d7.loss_dice: 3.0868, decode.d8.loss_cls: 0.0531, decode.d8.loss_mask: 1.3971, decode.d8.loss_dice: 3.0818, loss: 48.6081
2023-03-17 20:49:40,279 - mmseg - INFO - Iter [3850/4000]	lr: 5.814e-07, eta: 0:01:24, time: 0.535, data_time: 0.005, memory: 14565, decode.loss_cls: 0.0553, decode.loss_mask: 1.4055, decode.loss_dice: 3.0440, decode.d0.loss_cls: 3.3276, decode.d0.loss_mask: 1.4021, decode.d0.loss_dice: 3.0281, decode.d1.loss_cls: 0.0779, decode.d1.loss_mask: 1.4015, decode.d1.loss_dice: 3.0540, decode.d2.loss_cls: 0.0685, decode.d2.loss_mask: 1.4000, decode.d2.loss_dice: 3.0614, decode.d3.loss_cls: 0.0677, decode.d3.loss_mask: 1.3983, decode.d3.loss_dice: 3.0601, decode.d4.loss_cls: 0.0670, decode.d4.loss_mask: 1.3981, decode.d4.loss_dice: 3.0532, decode.d5.loss_cls: 0.0708, decode.d5.loss_mask: 1.3976, decode.d5.loss_dice: 3.0382, decode.d6.loss_cls: 0.0575, decode.d6.loss_mask: 1.4046, decode.d6.loss_dice: 3.0476, decode.d7.loss_cls: 0.0561, decode.d7.loss_mask: 1.4037, decode.d7.loss_dice: 3.0474, decode.d8.loss_cls: 0.0543, decode.d8.loss_mask: 1.4030, decode.d8.loss_dice: 3.0459, loss: 48.3970
2023-03-17 20:50:09,477 - mmseg - INFO - Iter [3900/4000]	lr: 3.889e-07, eta: 0:00:56, time: 0.584, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0395, decode.loss_mask: 1.4118, decode.loss_dice: 3.0668, decode.d0.loss_cls: 3.3466, decode.d0.loss_mask: 1.4126, decode.d0.loss_dice: 3.0322, decode.d1.loss_cls: 0.0658, decode.d1.loss_mask: 1.4139, decode.d1.loss_dice: 3.1057, decode.d2.loss_cls: 0.0457, decode.d2.loss_mask: 1.4142, decode.d2.loss_dice: 3.0804, decode.d3.loss_cls: 0.0389, decode.d3.loss_mask: 1.4126, decode.d3.loss_dice: 3.0533, decode.d4.loss_cls: 0.0383, decode.d4.loss_mask: 1.4105, decode.d4.loss_dice: 3.0753, decode.d5.loss_cls: 0.0422, decode.d5.loss_mask: 1.4122, decode.d5.loss_dice: 3.0670, decode.d6.loss_cls: 0.0389, decode.d6.loss_mask: 1.4085, decode.d6.loss_dice: 3.0774, decode.d7.loss_cls: 0.0387, decode.d7.loss_mask: 1.4122, decode.d7.loss_dice: 3.0837, decode.d8.loss_cls: 0.0424, decode.d8.loss_mask: 1.4096, decode.d8.loss_dice: 3.0771, loss: 48.5741
2023-03-17 20:50:36,906 - mmseg - INFO - Iter [3950/4000]	lr: 1.964e-07, eta: 0:00:28, time: 0.549, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0356, decode.loss_mask: 1.4427, decode.loss_dice: 3.0565, decode.d0.loss_cls: 3.3189, decode.d0.loss_mask: 1.4460, decode.d0.loss_dice: 3.0599, decode.d1.loss_cls: 0.0522, decode.d1.loss_mask: 1.4456, decode.d1.loss_dice: 3.0570, decode.d2.loss_cls: 0.0384, decode.d2.loss_mask: 1.4437, decode.d2.loss_dice: 3.0747, decode.d3.loss_cls: 0.0398, decode.d3.loss_mask: 1.4451, decode.d3.loss_dice: 3.0357, decode.d4.loss_cls: 0.0364, decode.d4.loss_mask: 1.4418, decode.d4.loss_dice: 3.0539, decode.d5.loss_cls: 0.0362, decode.d5.loss_mask: 1.4424, decode.d5.loss_dice: 3.0536, decode.d6.loss_cls: 0.0407, decode.d6.loss_mask: 1.4420, decode.d6.loss_dice: 3.0795, decode.d7.loss_cls: 0.0361, decode.d7.loss_mask: 1.4433, decode.d7.loss_dice: 3.0701, decode.d8.loss_cls: 0.0357, decode.d8.loss_mask: 1.4437, decode.d8.loss_dice: 3.0546, loss: 48.7018
2023-03-17 20:51:04,290 - mmseg - INFO - Saving checkpoint at 4000 iterations
2023-03-17 20:51:09,730 - mmseg - INFO - Exp name: mask2former_beit_adapter_base_480_40k_pascal_context_59_ss.py
2023-03-17 20:51:09,731 - mmseg - INFO - Iter [4000/4000]	lr: 3.850e-09, eta: 0:00:00, time: 0.656, data_time: 0.006, memory: 14565, decode.loss_cls: 0.0596, decode.loss_mask: 1.4634, decode.loss_dice: 3.0633, decode.d0.loss_cls: 3.3097, decode.d0.loss_mask: 1.4634, decode.d0.loss_dice: 3.0436, decode.d1.loss_cls: 0.0705, decode.d1.loss_mask: 1.4626, decode.d1.loss_dice: 3.0713, decode.d2.loss_cls: 0.0634, decode.d2.loss_mask: 1.4640, decode.d2.loss_dice: 3.0561, decode.d3.loss_cls: 0.0611, decode.d3.loss_mask: 1.4647, decode.d3.loss_dice: 3.0657, decode.d4.loss_cls: 0.0613, decode.d4.loss_mask: 1.4638, decode.d4.loss_dice: 3.0439, decode.d5.loss_cls: 0.0584, decode.d5.loss_mask: 1.4650, decode.d5.loss_dice: 3.0309, decode.d6.loss_cls: 0.0575, decode.d6.loss_mask: 1.4651, decode.d6.loss_dice: 3.0499, decode.d7.loss_cls: 0.0617, decode.d7.loss_mask: 1.4641, decode.d7.loss_dice: 3.0625, decode.d8.loss_cls: 0.0550, decode.d8.loss_mask: 1.4656, decode.d8.loss_dice: 3.0366, loss: 49.0237
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[                                                  ] 0/215, elapsed: 0s, ETA:[                                 ] 1/215, 0.6 task/s, elapsed: 2s, ETA:   360s[                                 ] 2/215, 1.0 task/s, elapsed: 2s, ETA:   216s[                                 ] 3/215, 1.3 task/s, elapsed: 2s, ETA:   168s[                                 ] 4/215, 1.5 task/s, elapsed: 3s, ETA:   143s[                                 ] 5/215, 1.6 task/s, elapsed: 3s, ETA:   129s[                                 ] 6/215, 1.8 task/s, elapsed: 3s, ETA:   119s[>                                ] 7/215, 1.9 task/s, elapsed: 4s, ETA:   112s[>                                ] 8/215, 1.9 task/s, elapsed: 4s, ETA:   106s[>                                ] 9/215, 2.0 task/s, elapsed: 4s, ETA:   102s[>                               ] 10/215, 2.1 task/s, elapsed: 5s, ETA:    99s[>                               ] 11/215, 2.1 task/s, elapsed: 5s, ETA:    96s[>                               ] 12/215, 2.2 task/s, elapsed: 5s, ETA:    93s[>                               ] 13/215, 2.2 task/s, elapsed: 6s, ETA:    91s[>>                              ] 14/215, 2.3 task/s, elapsed: 6s, ETA:    89s[>>                              ] 15/215, 2.3 task/s, elapsed: 7s, ETA:    87s[>>                              ] 16/215, 2.3 task/s, elapsed: 7s, ETA:    85s[>>                              ] 17/215, 2.4 task/s, elapsed: 7s, ETA:    84s[>>                              ] 18/215, 2.4 task/s, elapsed: 7s, ETA:    82s[>>                              ] 19/215, 2.4 task/s, elapsed: 8s, ETA:    81s[>>                              ] 20/215, 2.4 task/s, elapsed: 8s, ETA:    80s[>>>                             ] 21/215, 2.5 task/s, elapsed: 9s, ETA:    79s[>>>                             ] 22/215, 2.5 task/s, elapsed: 9s, ETA:    78s[>>>                             ] 23/215, 2.5 task/s, elapsed: 9s, ETA:    77s[>>>                            ] 24/215, 2.5 task/s, elapsed: 10s, ETA:    76s[>>>                            ] 25/215, 2.5 task/s, elapsed: 10s, ETA:    75s[>>>                            ] 26/215, 2.5 task/s, elapsed: 10s, ETA:    74s[>>>                            ] 27/215, 2.6 task/s, elapsed: 11s, ETA:    73s[>>>>                           ] 28/215, 2.6 task/s, elapsed: 11s, ETA:    73s[>>>>                           ] 29/215, 2.6 task/s, elapsed: 11s, ETA:    72s[>>>>                           ] 30/215, 2.6 task/s, elapsed: 12s, ETA:    71s[>>>>                           ] 31/215, 2.6 task/s, elapsed: 12s, ETA:    70s[>>>>                           ] 32/215, 2.6 task/s, elapsed: 12s, ETA:    70s[>>>>                           ] 33/215, 2.6 task/s, elapsed: 13s, ETA:    69s[>>>>                           ] 34/215, 2.6 task/s, elapsed: 13s, ETA:    69s[>>>>>                          ] 35/215, 2.6 task/s, elapsed: 13s, ETA:    68s[>>>>>                          ] 36/215, 2.6 task/s, elapsed: 14s, ETA:    68s[>>>>>                          ] 37/215, 2.6 task/s, elapsed: 14s, ETA:    67s[>>>>>                          ] 38/215, 2.7 task/s, elapsed: 14s, ETA:    67s[>>>>>                          ] 39/215, 2.7 task/s, elapsed: 15s, ETA:    66s[>>>>>                          ] 40/215, 2.7 task/s, elapsed: 15s, ETA:    65s[>>>>>                          ] 41/215, 2.7 task/s, elapsed: 15s, ETA:    65s[>>>>>>                         ] 42/215, 2.7 task/s, elapsed: 16s, ETA:    64s[>>>>>>                         ] 43/215, 2.7 task/s, elapsed: 16s, ETA:    64s[>>>>>>                         ] 44/215, 2.7 task/s, elapsed: 16s, ETA:    63s[>>>>>>                         ] 45/215, 2.7 task/s, elapsed: 17s, ETA:    63s[>>>>>>                         ] 46/215, 2.7 task/s, elapsed: 17s, ETA:    62s[>>>>>>                         ] 47/215, 2.7 task/s, elapsed: 17s, ETA:    62s[>>>>>>                         ] 48/215, 2.7 task/s, elapsed: 18s, ETA:    61s[>>>>>>>                        ] 49/215, 2.7 task/s, elapsed: 18s, ETA:    61s[>>>>>>>                        ] 50/215, 2.7 task/s, elapsed: 18s, ETA:    60s[>>>>>>>                        ] 51/215, 2.7 task/s, elapsed: 19s, ETA:    60s[>>>>>>>                        ] 52/215, 2.7 task/s, elapsed: 19s, ETA:    60s[>>>>>>>                        ] 53/215, 2.7 task/s, elapsed: 19s, ETA:    59s[>>>>>>>                        ] 54/215, 2.7 task/s, elapsed: 20s, ETA:    59s[>>>>>>>                        ] 55/215, 2.7 task/s, elapsed: 20s, ETA:    58s[>>>>>>>>                       ] 56/215, 2.7 task/s, elapsed: 20s, ETA:    58s[>>>>>>>>                       ] 57/215, 2.8 task/s, elapsed: 21s, ETA:    57s[>>>>>>>>                       ] 58/215, 2.8 task/s, elapsed: 21s, ETA:    57s[>>>>>>>>                       ] 59/215, 2.8 task/s, elapsed: 21s, ETA:    57s[>>>>>>>>                       ] 60/215, 2.8 task/s, elapsed: 22s, ETA:    56s[>>>>>>>>                       ] 61/215, 2.8 task/s, elapsed: 22s, ETA:    56s[>>>>>>>>                       ] 62/215, 2.8 task/s, elapsed: 22s, ETA:    55s[>>>>>>>>>                      ] 63/215, 2.8 task/s, elapsed: 23s, ETA:    55s[>>>>>>>>>                      ] 64/215, 2.8 task/s, elapsed: 23s, ETA:    54s[>>>>>>>>>                      ] 65/215, 2.8 task/s, elapsed: 23s, ETA:    54s[>>>>>>>>>                      ] 66/215, 2.8 task/s, elapsed: 24s, ETA:    54s[>>>>>>>>>                      ] 67/215, 2.8 task/s, elapsed: 24s, ETA:    53s[>>>>>>>>>                      ] 68/215, 2.8 task/s, elapsed: 24s, ETA:    53s[>>>>>>>>>                      ] 69/215, 2.8 task/s, elapsed: 25s, ETA:    52s[>>>>>>>>>>                     ] 70/215, 2.8 task/s, elapsed: 25s, ETA:    52s[>>>>>>>>>>                     ] 71/215, 2.8 task/s, elapsed: 25s, ETA:    52s[>>>>>>>>>>                     ] 72/215, 2.8 task/s, elapsed: 26s, ETA:    51s[>>>>>>>>>>                     ] 73/215, 2.8 task/s, elapsed: 26s, ETA:    51s[>>>>>>>>>>                     ] 74/215, 2.8 task/s, elapsed: 26s, ETA:    50s[>>>>>>>>>>                     ] 75/215, 2.8 task/s, elapsed: 27s, ETA:    50s[>>>>>>>>>>                     ] 76/215, 2.8 task/s, elapsed: 27s, ETA:    50s[>>>>>>>>>>>                    ] 77/215, 2.8 task/s, elapsed: 27s, ETA:    49s[>>>>>>>>>>>                    ] 78/215, 2.8 task/s, elapsed: 28s, ETA:    49s[>>>>>>>>>>>                    ] 79/215, 2.8 task/s, elapsed: 28s, ETA:    49s[>>>>>>>>>>>                    ] 80/215, 2.8 task/s, elapsed: 29s, ETA:    48s[>>>>>>>>>>>                    ] 81/215, 2.8 task/s, elapsed: 29s, ETA:    48s[>>>>>>>>>>>                    ] 82/215, 2.8 task/s, elapsed: 29s, ETA:    48s[>>>>>>>>>>>                    ] 83/215, 2.8 task/s, elapsed: 30s, ETA:    47s[>>>>>>>>>>>>                   ] 84/215, 2.8 task/s, elapsed: 30s, ETA:    47s[>>>>>>>>>>>>                   ] 85/215, 2.8 task/s, elapsed: 31s, ETA:    47s[>>>>>>>>>>>>                   ] 86/215, 2.8 task/s, elapsed: 31s, ETA:    46s[>>>>>>>>>>>>                   ] 87/215, 2.8 task/s, elapsed: 31s, ETA:    46s[>>>>>>>>>>>>                   ] 88/215, 2.8 task/s, elapsed: 32s, ETA:    46s[>>>>>>>>>>>>                   ] 89/215, 2.8 task/s, elapsed: 32s, ETA:    45s[>>>>>>>>>>>>                   ] 90/215, 2.8 task/s, elapsed: 32s, ETA:    45s[>>>>>>>>>>>>>                  ] 91/215, 2.8 task/s, elapsed: 33s, ETA:    44s[>>>>>>>>>>>>>                  ] 92/215, 2.8 task/s, elapsed: 33s, ETA:    44s[>>>>>>>>>>>>>                  ] 93/215, 2.8 task/s, elapsed: 33s, ETA:    44s[>>>>>>>>>>>>>                  ] 94/215, 2.8 task/s, elapsed: 34s, ETA:    43s[>>>>>>>>>>>>>                  ] 95/215, 2.8 task/s, elapsed: 34s, ETA:    43s[>>>>>>>>>>>>>                  ] 96/215, 2.8 task/s, elapsed: 34s, ETA:    43s[>>>>>>>>>>>>>                  ] 97/215, 2.8 task/s, elapsed: 35s, ETA:    42s[>>>>>>>>>>>>>>                 ] 98/215, 2.8 task/s, elapsed: 35s, ETA:    42s[>>>>>>>>>>>>>>                 ] 99/215, 2.8 task/s, elapsed: 35s, ETA:    42s[>>>>>>>>>>>>>                 ] 100/215, 2.8 task/s, elapsed: 36s, ETA:    41s[>>>>>>>>>>>>>>                ] 101/215, 2.8 task/s, elapsed: 36s, ETA:    41s[>>>>>>>>>>>>>>                ] 102/215, 2.8 task/s, elapsed: 36s, ETA:    40s[>>>>>>>>>>>>>>                ] 103/215, 2.8 task/s, elapsed: 37s, ETA:    40s[>>>>>>>>>>>>>>                ] 104/215, 2.8 task/s, elapsed: 37s, ETA:    40s[>>>>>>>>>>>>>>                ] 105/215, 2.8 task/s, elapsed: 37s, ETA:    39s[>>>>>>>>>>>>>>                ] 106/215, 2.8 task/s, elapsed: 38s, ETA:    39s[>>>>>>>>>>>>>>                ] 107/215, 2.8 task/s, elapsed: 38s, ETA:    39s[>>>>>>>>>>>>>>>               ] 108/215, 2.8 task/s, elapsed: 38s, ETA:    38s[>>>>>>>>>>>>>>>               ] 109/215, 2.8 task/s, elapsed: 39s, ETA:    38s[>>>>>>>>>>>>>>>               ] 110/215, 2.8 task/s, elapsed: 39s, ETA:    37s[>>>>>>>>>>>>>>>               ] 111/215, 2.8 task/s, elapsed: 40s, ETA:    37s[>>>>>>>>>>>>>>>               ] 112/215, 2.8 task/s, elapsed: 40s, ETA:    37s[>>>>>>>>>>>>>>>               ] 113/215, 2.8 task/s, elapsed: 40s, ETA:    36s[>>>>>>>>>>>>>>>               ] 114/215, 2.8 task/s, elapsed: 41s, ETA:    36s[>>>>>>>>>>>>>>>>              ] 115/215, 2.8 task/s, elapsed: 41s, ETA:    36s[>>>>>>>>>>>>>>>>              ] 116/215, 2.8 task/s, elapsed: 41s, ETA:    35s[>>>>>>>>>>>>>>>>              ] 117/215, 2.8 task/s, elapsed: 42s, ETA:    35s[>>>>>>>>>>>>>>>>              ] 118/215, 2.8 task/s, elapsed: 42s, ETA:    34s[>>>>>>>>>>>>>>>>              ] 119/215, 2.8 task/s, elapsed: 42s, ETA:    34s[>>>>>>>>>>>>>>>>              ] 120/215, 2.8 task/s, elapsed: 43s, ETA:    34s[>>>>>>>>>>>>>>>>              ] 121/215, 2.8 task/s, elapsed: 43s, ETA:    33s[>>>>>>>>>>>>>>>>>             ] 122/215, 2.8 task/s, elapsed: 43s, ETA:    33s[>>>>>>>>>>>>>>>>>             ] 123/215, 2.8 task/s, elapsed: 44s, ETA:    33s[>>>>>>>>>>>>>>>>>             ] 124/215, 2.8 task/s, elapsed: 44s, ETA:    32s[>>>>>>>>>>>>>>>>>             ] 125/215, 2.8 task/s, elapsed: 44s, ETA:    32s[>>>>>>>>>>>>>>>>>             ] 126/215, 2.8 task/s, elapsed: 45s, ETA:    31s[>>>>>>>>>>>>>>>>>             ] 127/215, 2.8 task/s, elapsed: 45s, ETA:    31s[>>>>>>>>>>>>>>>>>             ] 128/215, 2.8 task/s, elapsed: 45s, ETA:    31s[>>>>>>>>>>>>>>>>>>            ] 129/215, 2.8 task/s, elapsed: 46s, ETA:    30s[>>>>>>>>>>>>>>>>>>            ] 130/215, 2.8 task/s, elapsed: 46s, ETA:    30s[>>>>>>>>>>>>>>>>>>            ] 131/215, 2.8 task/s, elapsed: 46s, ETA:    30s[>>>>>>>>>>>>>>>>>>            ] 132/215, 2.8 task/s, elapsed: 47s, ETA:    29s[>>>>>>>>>>>>>>>>>>            ] 133/215, 2.8 task/s, elapsed: 47s, ETA:    29s[>>>>>>>>>>>>>>>>>>            ] 134/215, 2.8 task/s, elapsed: 47s, ETA:    29s[>>>>>>>>>>>>>>>>>>            ] 135/215, 2.8 task/s, elapsed: 48s, ETA:    28s[>>>>>>>>>>>>>>>>>>            ] 136/215, 2.8 task/s, elapsed: 48s, ETA:    28s[>>>>>>>>>>>>>>>>>>>           ] 137/215, 2.8 task/s, elapsed: 48s, ETA:    27s[>>>>>>>>>>>>>>>>>>>           ] 138/215, 2.8 task/s, elapsed: 49s, ETA:    27s[>>>>>>>>>>>>>>>>>>>           ] 139/215, 2.8 task/s, elapsed: 49s, ETA:    27s[>>>>>>>>>>>>>>>>>>>           ] 140/215, 2.8 task/s, elapsed: 49s, ETA:    26s[>>>>>>>>>>>>>>>>>>>           ] 141/215, 2.8 task/s, elapsed: 50s, ETA:    26s[>>>>>>>>>>>>>>>>>>>           ] 142/215, 2.8 task/s, elapsed: 50s, ETA:    26s[>>>>>>>>>>>>>>>>>>>           ] 143/215, 2.8 task/s, elapsed: 50s, ETA:    25s[>>>>>>>>>>>>>>>>>>>>          ] 144/215, 2.9 task/s, elapsed: 51s, ETA:    25s[>>>>>>>>>>>>>>>>>>>>          ] 145/215, 2.8 task/s, elapsed: 51s, ETA:    25s[>>>>>>>>>>>>>>>>>>>>          ] 146/215, 2.9 task/s, elapsed: 51s, ETA:    24s[>>>>>>>>>>>>>>>>>>>>          ] 147/215, 2.9 task/s, elapsed: 52s, ETA:    24s[>>>>>>>>>>>>>>>>>>>>          ] 148/215, 2.9 task/s, elapsed: 52s, ETA:    23s[>>>>>>>>>>>>>>>>>>>>          ] 149/215, 2.9 task/s, elapsed: 52s, ETA:    23s[>>>>>>>>>>>>>>>>>>>>          ] 150/215, 2.9 task/s, elapsed: 53s, ETA:    23s[>>>>>>>>>>>>>>>>>>>>>         ] 151/215, 2.9 task/s, elapsed: 53s, ETA:    22s[>>>>>>>>>>>>>>>>>>>>>         ] 152/215, 2.9 task/s, elapsed: 53s, ETA:    22s[>>>>>>>>>>>>>>>>>>>>>         ] 153/215, 2.9 task/s, elapsed: 54s, ETA:    22s[>>>>>>>>>>>>>>>>>>>>>         ] 154/215, 2.9 task/s, elapsed: 54s, ETA:    21s[>>>>>>>>>>>>>>>>>>>>>         ] 155/215, 2.9 task/s, elapsed: 54s, ETA:    21s[>>>>>>>>>>>>>>>>>>>>>         ] 156/215, 2.9 task/s, elapsed: 55s, ETA:    21s[>>>>>>>>>>>>>>>>>>>>>         ] 157/215, 2.9 task/s, elapsed: 55s, ETA:    20s[>>>>>>>>>>>>>>>>>>>>>>        ] 158/215, 2.9 task/s, elapsed: 55s, ETA:    20s[>>>>>>>>>>>>>>>>>>>>>>        ] 159/215, 2.9 task/s, elapsed: 56s, ETA:    20s[>>>>>>>>>>>>>>>>>>>>>>        ] 160/215, 2.9 task/s, elapsed: 56s, ETA:    19s[>>>>>>>>>>>>>>>>>>>>>>        ] 161/215, 2.9 task/s, elapsed: 56s, ETA:    19s[>>>>>>>>>>>>>>>>>>>>>>        ] 162/215, 2.9 task/s, elapsed: 57s, ETA:    19s[>>>>>>>>>>>>>>>>>>>>>>        ] 163/215, 2.9 task/s, elapsed: 57s, ETA:    18s[>>>>>>>>>>>>>>>>>>>>>>        ] 164/215, 2.9 task/s, elapsed: 57s, ETA:    18s[>>>>>>>>>>>>>>>>>>>>>>>       ] 165/215, 2.9 task/s, elapsed: 58s, ETA:    17s[>>>>>>>>>>>>>>>>>>>>>>>       ] 166/215, 2.9 task/s, elapsed: 58s, ETA:    17s[>>>>>>>>>>>>>>>>>>>>>>>       ] 167/215, 2.9 task/s, elapsed: 58s, ETA:    17s[>>>>>>>>>>>>>>>>>>>>>>>       ] 168/215, 2.9 task/s, elapsed: 59s, ETA:    16s[>>>>>>>>>>>>>>>>>>>>>>>       ] 169/215, 2.9 task/s, elapsed: 59s, ETA:    16s[>>>>>>>>>>>>>>>>>>>>>>>       ] 170/215, 2.9 task/s, elapsed: 59s, ETA:    16s[>>>>>>>>>>>>>>>>>>>>>>>       ] 171/215, 2.9 task/s, elapsed: 59s, ETA:    15s[>>>>>>>>>>>>>>>>>>>>>>>>      ] 172/215, 2.9 task/s, elapsed: 60s, ETA:    15s[>>>>>>>>>>>>>>>>>>>>>>>>      ] 173/215, 2.9 task/s, elapsed: 60s, ETA:    15s[>>>>>>>>>>>>>>>>>>>>>>>>      ] 174/215, 2.9 task/s, elapsed: 60s, ETA:    14s[>>>>>>>>>>>>>>>>>>>>>>>>      ] 175/215, 2.9 task/s, elapsed: 61s, ETA:    14s[>>>>>>>>>>>>>>>>>>>>>>>>      ] 176/215, 2.9 task/s, elapsed: 61s, ETA:    14s[>>>>>>>>>>>>>>>>>>>>>>>>      ] 177/215, 2.9 task/s, elapsed: 61s, ETA:    13s[>>>>>>>>>>>>>>>>>>>>>>>>      ] 178/215, 2.9 task/s, elapsed: 62s, ETA:    13s[>>>>>>>>>>>>>>>>>>>>>>>>      ] 179/215, 2.9 task/s, elapsed: 62s, ETA:    12s[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 180/215, 2.9 task/s, elapsed: 62s, ETA:    12s[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 181/215, 2.9 task/s, elapsed: 63s, ETA:    12s[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 182/215, 2.9 task/s, elapsed: 63s, ETA:    11s[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 183/215, 2.9 task/s, elapsed: 63s, ETA:    11s[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 184/215, 2.9 task/s, elapsed: 64s, ETA:    11s[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 185/215, 2.9 task/s, elapsed: 64s, ETA:    10s[>>>>>>>>>>>>>>>>>>>>>>>>>     ] 186/215, 2.9 task/s, elapsed: 64s, ETA:    10s[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 187/215, 2.9 task/s, elapsed: 65s, ETA:    10s[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 188/215, 2.9 task/s, elapsed: 65s, ETA:     9s[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 189/215, 2.9 task/s, elapsed: 65s, ETA:     9s[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 190/215, 2.9 task/s, elapsed: 66s, ETA:     9s[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 191/215, 2.9 task/s, elapsed: 66s, ETA:     8s[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 192/215, 2.9 task/s, elapsed: 66s, ETA:     8s[>>>>>>>>>>>>>>>>>>>>>>>>>>    ] 193/215, 2.9 task/s, elapsed: 67s, ETA:     8s[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 194/215, 2.9 task/s, elapsed: 67s, ETA:     7s[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 195/215, 2.9 task/s, elapsed: 67s, ETA:     7s[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 196/215, 2.9 task/s, elapsed: 68s, ETA:     7s[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 197/215, 2.9 task/s, elapsed: 68s, ETA:     6s[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 198/215, 2.9 task/s, elapsed: 68s, ETA:     6s[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 199/215, 2.9 task/s, elapsed: 69s, ETA:     6s[>>>>>>>>>>>>>>>>>>>>>>>>>>>   ] 200/215, 2.9 task/s, elapsed: 69s, ETA:     5s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 201/215, 2.9 task/s, elapsed: 69s, ETA:     5s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 202/215, 2.9 task/s, elapsed: 70s, ETA:     4s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 203/215, 2.9 task/s, elapsed: 70s, ETA:     4s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 204/215, 2.9 task/s, elapsed: 70s2023-03-17 20:52:23,871 - mmseg - INFO - per class results:
2023-03-17 20:52:23,876 - mmseg - INFO - 
+-----------------+-------+-------+
|      Class      |  IoU  |  Acc  |
+-----------------+-------+-------+
|      cloudy     | 81.01 | 95.37 |
| uncertain clear | 10.41 | 12.88 |
|  probably clear | 28.18 | 40.27 |
| confident clear | 55.51 | 74.93 |
+-----------------+-------+-------+
2023-03-17 20:52:23,878 - mmseg - INFO - Summary:
2023-03-17 20:52:23,881 - mmseg - INFO - 
+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 75.99 | 43.78 | 55.86 |
+-------+-------+-------+
2023-03-17 20:52:29,099 - mmseg - INFO - Now best checkpoint is saved as best_mIoU_iter_4000.pth.
2023-03-17 20:52:29,101 - mmseg - INFO - Best mIoU is 0.4378 at 4000 iter.
2023-03-17 20:52:29,104 - mmseg - INFO - Exp name: mask2former_beit_adapter_base_480_40k_pascal_context_59_ss.py
2023-03-17 20:52:29,105 - mmseg - INFO - Iter(val) [215]	aAcc: 0.7599, mIoU: 0.4378, mAcc: 0.5586, IoU.cloudy: 0.8101, IoU.uncertain clear: 0.1041, IoU.probably clear: 0.2818, IoU.confident clear: 0.5551, Acc.cloudy: 0.9537, Acc.uncertain clear: 0.1288, Acc.probably clear: 0.4027, Acc.confident clear: 0.7493
, ETA:     4s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 205/215, 2.9 task/s, elapsed: 71s, ETA:     3s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 206/215, 2.9 task/s, elapsed: 71s, ETA:     3s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 207/215, 2.9 task/s, elapsed: 71s, ETA:     3s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 208/215, 2.9 task/s, elapsed: 72s, ETA:     2s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 209/215, 2.9 task/s, elapsed: 72s, ETA:     2s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 210/215, 2.9 task/s, elapsed: 72s, ETA:     2s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 211/215, 2.9 task/s, elapsed: 73s, ETA:     1s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 212/215, 2.9 task/s, elapsed: 73s, ETA:     1s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 213/215, 2.9 task/s, elapsed: 73s, ETA:     1s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 214/215, 2.9 task/s, elapsed: 74s, ETA:     0s[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 215/215, 2.9 task/s, elapsed: 74s, ETA:     0s